%title  How MapReduce Works
%toc


当前日期: 01/30/2013 Wed 
=How MapReduce Works=

==MapReduce工作机制 ==

===经典的MapReduce(MapReduce 1) ===
{{mapreduce_classic.png}}

*参与者:*
* *Client:*, MapReduce Job的提交者
* *Jobtracker:*, 负责安排Job的运行，对应的 class 是 JobTracker.
* *Tasktrackers:* 负责运行由Job分解出的Task, 对应的 class 是 TaskTracker.
* *Distributed Filesystem:* 通常HDFS,用于共享Job相关的文件

*提交Job:*
# *Step 1:* 创建一个JobSubmitter实例，并调用其方法submitJobInternal(). (由Job的方法submit()触发)
# 方法`waitForCompletion()`会每秒拉一次Job的状态与进度
# 提交的具体步骤：
  # *Step 2:* 向jobtracker请求一个新的job ID (`getNewJobId()`)
  # 查看job的输出设置条件是否满足，比如输出文件夹是否存在，不存在不能运行
  # 计算输入的分片，不能分片不能运行，比如输入路径不存在
  # *Step 3:* 拷贝job的相关资源到jobtracker的文件系统（jar,configuration,分片结果等），jar文件会有很多份
    * 通过属性 `mapred.submit.replication` 指定，默认为10
    * 当运行Task时，可以很快速从集群中获得
  # *Step 4:* 告诉Jobtracker,job已准备好运行（调用JobTracker的submitJob() ).

*初始化Job:*
# Jobtracker收到submitJob()的请求后，会把job放入一个队列里
# job scheduler会从队列里取出并初始化
# *Step 5:* 创建一个代表job的对象
  * 包含task信息
  * 进度和状态信息
# *Step 6:* 接受由client计算出的分片信息，创建要运行的task列表
  * 为每一个分片创建一个map task
  * 属性 `mapred.reduce.tasks` 确定reduce task的数目
  * 此时所有的task都会被分配id
  * 还有两个额外的task会被创建：job setup task 和 job clean up task
    * 用于在任何map　task前启动一个job
    * 用于在所有reduce task后，清理工作

*分配Task:*
# *Step 7:* task tracker 会不间隔向job tracker发送心跳heart beat(使用一个循环体）
  * 告诉job tracker,它还活着
  * 也可以用于交换信息
  * 比如告诉job　tracker,已经准备好接收新的task, job tracker 就会分配给它一个新的任务
# task tracker 会有一定数量的map task和reduce task的槽位
  * 这两种槽位是相互独立的
  * 具体的数量由cpu 核数和内存决定
  * task tracker 会优先使用map task的槽位
# 对于reduce task, job tracker只是简单的取出运行
# 对于map task, job tracker会尽量选取距数据最近的task tracker运行，最好在同一个data node 上

*运行Task:*
# task tracker 分配到一个task, 接下来就是运行这个task
# *Step 8:* 接收所需的文件
  # 将job jar和所需要的一切文件拷到本地系统
  # 为task创建一个本地目录，并将jar解压
  # 创建一个　TaskRunner 实例，用于运行task
# *Step 9:* 启动一个新的JVM
# *Step 10:* 运行task
# 子进程会每隔几秒钟向父进程回报一下task进度，直到task完成
# 每一个task都会运行setup和cleanup操作

*进度与状态报告:*
# 当一个task运行时，会记录它的进度
# 当task进度发生变化时，会设置一个flag,表明需要向task tracker汇报进度
# 有一个独立线程会每隔３s检查一个这个flag,如果需要就通知task tracker当前的进度
# task tracker会每５s向job tracker发送一个心跳，如果有信息会随着一起发送
# client的job每秒向job tracker拉一次状态
{{mapreduce_classic_progress.png}}

*Job完成:*
# job tracker收到最后一个task的完成通知后(cleanup task)
# 将job设为成功
# 属性 `job.end.notification.url` 可以配置一个需要http通知的url

===二代:Yarn(MapReduce 2) ===
在4000个结点以上的集群中，上述方案的水平扩展就会遇到瓶颈，所以有了YARN:
* Yet Another Resource Negotiator
* YARN Application Resource Negotiator

# YARN通过分解Jobtracker的工作来达到扩大水平扩展能力的手段。jobtracker 的两项职责：
  # job scheduling 分配task给tasktracker
  # 监视task的执行进度
# YARN将这两项职责分给两个独立的后台进程：
  * *Resource Manager:* 管理集群上资源的使用
  * *Application Master:* 管理运行于集群上的application的生命周期，application对应job 
  
*基本思路:*
# 此处的application就是mapreduce job
# `application master` 与 `resource manager` 协商集群上的资源
# 资源表现为容器 `containers` , 它有一定的内存限制
# application的进程就运行于这些 `containers` 上
# 这些 `containers` 会被运行于集群结点上的  `node manager` 监管，避免application使用超过分配给它的资源
# 目前集群上的资源就是内存

*与jobtracker相比，每一个application拥有自己的* `application master` , *运行于application的期间，application就是mapreduce job*
  
{{mapreduce_yarn.png}}

*参与者:*
* *Client:*, MapReduce Job的提交者
* *YARN resource manager:* 管理协调集群上的计算资源
* *YARN node managers:*  启动并监视其所在结点上的计算容器
* *MapReduce application master:* 管理协调一个mapreduce job的所有task. application master 和 job的 task运行于由 `resource manager` 调度安排并由 `node manager` 管理的计算容器中
* *Distributed Filesystem:* 通常HDFS,用于共享Job相关的文件
 
*提交Job:*
# *Step 1:* 创建一个JobSubmitter实例，并调用其方法submitJobInternal(). (由Job的方法submit()触发)
  * 和MapReduce 1使用一样的api
  * 通过属性 `mapreduce.framework.name` 设置为yarn激活YARN
# *Step 2:* 从 `resource manager` 获取一个新的application id
# client 计算分片
  * 分片也可以在集群中计算
  * 通过属性 `yarn.app.mapreduce.am.compute-splits-in-cluster` 激活
# *Step 3:* 拷贝application的相关资源到HDFS（jar,configuration,分片结果等），jar文件会有很多份
# *Step 4:* 在 `resource manager` 调用 submitApplication() 提交一个application

*初始化Job:*
# *Step 5a:* scheduler 分配一个 `container`
# *Step 5b:* `resource manager` 在这个 `container` 上运行 `application master`, 并在 `node manager` 的管理下
# *Step 6:* `applicaiton master` 初始化一个application
  * `applicaiton master` 对应类  MRAppMaster
  * 创建一系列对象追踪job的进度 
  * 稍后它会从task处得到进度和完成信息
# *Step 7:* `applicaiton master` 从分布式系统中获取计算好的splits
#  `application master` 创建tasks:
  * 为每一个split创建一个map task
  * 创建redeuce task, 数量由属性 `mapreduce.job.reduces` 指定
#  如果是一个小的job,`application master` 将会在它自身的jvm上运行些task,如果判定这是一个小的job :
  * `mapreduce.job.ubertask.maxmaps` 默认值10
  * `mapreduce.job.ubertask.maxreduces` 默认值1
  * `mapreduce.job.ubertask.maxbytes` 默认值HDFS Block
  * `mapreduce.job.ubertask.enable` 默认值true

*分配Task:*
# *Step 7:* `applicaiton master` 向 `resource manager` 申请运行map task和reduce task所需的资源 `containers`
# 为task分配内存,通过下列属性来指定
  * `mapreduce.map.memory.mb` map task的内存  默认:1024M
  * `mapreduce.reduce.memory.mb` reduce task的内存   默认:1024M
  * `yarn.scheduler.capacity.minimum-allocation-mb` 最小值限制，默认为1G
  * `yarn.scheduler.capacity.maximum-allocation-mb`  最大值限制，默认为10G
  * 分配给task的内存必须在最小值和最大值之间，且是最小值的整数倍
  * 如果不是整数倍，会自动上升为整数倍

*运行Task:*
# task被分配了一个资源 `container` 
# *Step 9a:* `application master` 通知相应的 `node manager` 运行这个 `container`
# *Step 9b:* 通过类YarnChild启动一个新jvm
# *Step 10:* 从分布式缓存中获取运行task所需的资源：jar configuration etc.
# *Step 11:* 运行这个task

*进度与状态报告:*
# task向 `application master` 汇报进度与状态
# `application master` 每三秒绘聚一次整个job的概况
# client 每秒向 `application master` 拉取一次job的进度与状态
  * 可能通过属性 `mapreduce.client.progressmonitor.pollinterval` 修改这个时间值 
{{mapreduce_yarn_report.png}}

*Job完成:*
# client会向 `application master` 轮询job是否完成
  * 通过调用Job的方法waitForCompletion()
  * 间隔时间通过属性 `mapreduce.client.completion.pollinterval` 指定
# 一旦完成：
  # `application master` 和 `containers` 执行cleanup操作
  # 调用OutputCommiter的cleanup方法
  # job的信息被job history server归档

==处理失败 ==

==Job的调度 ==

==Shuffle和排序 ==

==Task的运行 ==
