<!DOCTYPE html>
<html>
<head>

<title> How MapReduce Works</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">

<script type="text/javascript" src="../../js/jquery-1.6.4.min.js"></script>

<link rel="Stylesheet" type="text/css" href="../../js/sh/styles/shCore.css">
<link rel="Stylesheet" type="text/css" href="../../js/sh/styles/shThemeRDark.css">
<script type="text/javascript" src="../../js/sh/scripts/shCore.js"></script>
<script type="text/javascript" src="../../js/sh/scripts/shAutoloader.js"></script>

<script type="text/javascript" src="../../js/main.js"></script>
<script type="text/javascript" src="../../js/ASCIIMathML.js"></script>

<link rel="Stylesheet" type="text/css" href="../../style.css">
<link rel="Stylesheet" type="text/css" href="../../css/main.css">

</head>
<body>
	<div class="hidden">
		<input id="root_path" type="text" value="../../">
	</div>
	<div id="body-wrapper">
		<div id="container">
			<div id="top">
				<div id="page-title">
					<a href="../../index.html">烂笔头</a>
				</div>
				<ul id="top-nav">
				</ul>
			</div>
			<div id="middle">
				
<div class="toc">
<ul>
<li><a href="#toc_1">How MapReduce Works</a>
<ul>
<li><a href="#toc_1.1">MapReduce工作机制</a>
<ul>
<li><a href="#toc_1.1.1">经典的MapReduce(MapReduce 1)</a>
<li><a href="#toc_1.1.2">二代:Yarn(MapReduce 2)</a>
</ul>
<li><a href="#toc_1.2">处理失败</a>
<ul>
<li><a href="#toc_1.2.1">经典MapReduce(MapReduce 1)的失败</a>
<li><a href="#toc_1.2.2">YARN(MapReduce 2)中的失败</a>
</ul>
<li><a href="#toc_1.3">Job的调度</a>
<ul>
<li><a href="#toc_1.3.1">公平调度</a>
<li><a href="#toc_1.3.2">容量调度</a>
</ul>
<li><a href="#toc_1.4">Shuffle和排序</a>
<ul>
<li><a href="#toc_1.4.1">Map阶段</a>
<li><a href="#toc_1.4.2">Reduce阶段</a>
<li><a href="#toc_1.4.3">配置调整</a>
</ul>
<li><a href="#toc_1.5">Task的运行</a>
<ul>
<li><a href="#toc_1.5.1">Speculative Task 推测式任务</a>
<li><a href="#toc_1.5.2">跳过不合法记录</a>
</ul>
</ul>
</ul>
</div>


<p>
当前日期: 01/30/2013 Wed 
</p>
<h1 id="toc_1">How MapReduce Works</h1>

<h2 id="toc_1.1">MapReduce工作机制</h2>

<h3 id="toc_1.1.1">经典的MapReduce(MapReduce 1)</h3>
<p>
<img src="mapreduce_classic.png" />
</p>

<p>
<strong>参与者:</strong>
</p>
<ul>
<li>
<strong>Client:</strong>, MapReduce Job的提交者

<li>
<strong>Jobtracker:</strong>, 负责安排Job的运行，对应的 class 是 JobTracker.

<li>
<strong>Tasktrackers:</strong> 负责运行由Job分解出的Task, 对应的 class 是 TaskTracker.

<li>
<strong>Distributed Filesystem:</strong> 通常HDFS,用于共享Job相关的文件

</ul>

<p>
<strong>提交Job:</strong>
</p>
<ol>
<li>
<strong>Step 1:</strong> 创建一个JobSubmitter实例，并调用其方法submitJobInternal(). (由Job的方法submit()触发)

<li>
方法<code>waitForCompletion()</code>会每秒拉一次Job的状态与进度

<li>
提交的具体步骤：

<ol>
<li>
<strong>Step 2:</strong> 向jobtracker请求一个新的job ID (<code>getNewJobId()</code>)

<li>
查看job的输出设置条件是否满足，比如输出文件夹是否存在，不存在不能运行

<li>
计算输入的分片，不能分片不能运行，比如输入路径不存在

<li>
<strong>Step 3:</strong> 拷贝job的相关资源到jobtracker的文件系统（jar,configuration,分片结果等），jar文件会有很多份

<ul>
<li>
通过属性 <code>mapred.submit.replication</code> 指定，默认为10

<li>
当运行Task时，可以很快速从集群中获得

</ul>
<li>
<strong>Step 4:</strong> 告诉Jobtracker,job已准备好运行（调用JobTracker的submitJob() ).

</ol>
</ol>

<p>
<strong>初始化Job:</strong>
</p>
<ol>
<li>
Jobtracker收到submitJob()的请求后，会把job放入一个队列里

<li>
job scheduler会从队列里取出并初始化

<li>
<strong>Step 5:</strong> 创建一个代表job的对象

<ul>
<li>
包含task信息

<li>
进度和状态信息

</ul>
<li>
<strong>Step 6:</strong> 接受由client计算出的分片信息，创建要运行的task列表

<ul>
<li>
为每一个分片创建一个map task

<li>
属性 <code>mapred.reduce.tasks</code> 确定reduce task的数目

<li>
此时所有的task都会被分配id

<li>
还有两个额外的task会被创建：job setup task 和 job clean up task

<ul>
<li>
用于在任何map　task前启动一个job

<li>
用于在所有reduce task后，清理工作

</ul>
</ul>
</ol>

<p>
<strong>分配Task:</strong>
</p>
<ol>
<li>
<strong>Step 7:</strong> task tracker 会不间隔向job tracker发送心跳heart beat(使用一个循环体）

<ul>
<li>
告诉job tracker,它还活着

<li>
也可以用于交换信息

<li>
比如告诉job　tracker,已经准备好接收新的task, job tracker 就会分配给它一个新的任务

</ul>
<li>
task tracker 会有一定数量的map task和reduce task的槽位

<ul>
<li>
这两种槽位是相互独立的

<li>
具体的数量由cpu 核数和内存决定

<li>
task tracker 会优先使用map task的槽位

</ul>
<li>
对于reduce task, job tracker只是简单的取出运行

<li>
对于map task, job tracker会尽量选取距数据最近的task tracker运行，最好在同一个data node 上

</ol>

<p>
<strong>运行Task:</strong>
</p>
<ol>
<li>
task tracker 分配到一个task, 接下来就是运行这个task

<li>
<strong>Step 8:</strong> 接收所需的文件

<ol>
<li>
将job jar和所需要的一切文件拷到本地系统

<li>
为task创建一个本地目录，并将jar解压

<li>
创建一个　TaskRunner 实例，用于运行task

</ol>
<li>
<strong>Step 9:</strong> 启动一个新的JVM

<li>
<strong>Step 10:</strong> 运行task

<li>
子进程会每隔几秒钟向父进程回报一下task进度，直到task完成

<li>
每一个task都会运行setup和cleanup操作

</ol>

<p>
<strong>进度与状态报告:</strong>
</p>
<ol>
<li>
当一个task运行时，会记录它的进度

<li>
当task进度发生变化时，会设置一个flag,表明需要向task tracker汇报进度

<li>
有一个独立线程会每隔３s检查一个这个flag,如果需要就通知task tracker当前的进度

<li>
task tracker会每５s向job tracker发送一个心跳，如果有信息会随着一起发送

<li>
client的job每秒向job tracker拉一次状态

</ol>
<p>
<img src="mapreduce_classic_progress.png" />
</p>

<p>
<strong>Job完成:</strong>
</p>
<ol>
<li>
job tracker收到最后一个task的完成通知后(cleanup task)

<li>
将job设为成功

<li>
属性 <code>job.end.notification.url</code> 可以配置一个需要http通知的url

</ol>

<h3 id="toc_1.1.2">二代:Yarn(MapReduce 2)</h3>
<p>
在4000个结点以上的集群中，上述方案的水平扩展就会遇到瓶颈，所以有了YARN:
</p>
<ul>
<li>
Yet Another Resource Negotiator

<li>
YARN Application Resource Negotiator

</ul>

<ol>
<li>
YARN通过分解Jobtracker的工作来达到扩大水平扩展能力的手段。jobtracker 的两项职责：

<ol>
<li>
job scheduling 分配task给tasktracker

<li>
监视task的执行进度

</ol>
<li>
YARN将这两项职责分给两个独立的后台进程：

<ul>
<li>
<strong>Resource Manager:</strong> 管理集群上资源的使用

<li>
<strong>Application Master:</strong> 管理运行于集群上的application的生命周期，application对应job 

</ul>
</ol>
  
<p>
<strong>基本思路:</strong>
</p>
<ol>
<li>
此处的application就是mapreduce job

<li>
<code>application master</code> 与 <code>resource manager</code> 协商集群上的资源

<li>
资源表现为容器 <code>containers</code> , 它有一定的内存限制

<li>
application的进程就运行于这些 <code>containers</code> 上

<li>
这些 <code>containers</code> 会被运行于集群结点上的  <code>node manager</code> 监管，避免application使用超过分配给它的资源

<li>
目前集群上的资源就是内存

</ol>

<p>
<strong>与jobtracker相比，每一个application拥有自己的</strong> <code>application master</code> , <strong>运行于application的期间，application就是mapreduce job</strong>
</p>
  
<p>
<img src="mapreduce_yarn.png" />
</p>

<p>
<strong>参与者:</strong>
</p>
<ul>
<li>
<strong>Client:</strong>, MapReduce Job的提交者

<li>
<strong>YARN resource manager:</strong> 管理协调集群上的计算资源

<li>
<strong>YARN node managers:</strong>  启动并监视其所在结点上的计算容器

<li>
<strong>MapReduce application master:</strong> 管理协调一个mapreduce job的所有task. application master 和 job的 task运行于由 <code>resource manager</code> 调度安排并由 <code>node manager</code> 管理的计算容器中

<li>
<strong>Distributed Filesystem:</strong> 通常HDFS,用于共享Job相关的文件

</ul>
 
<p>
<strong>提交Job:</strong>
</p>
<ol>
<li>
<strong>Step 1:</strong> 创建一个JobSubmitter实例，并调用其方法submitJobInternal(). (由Job的方法submit()触发)

<ul>
<li>
和MapReduce 1使用一样的api

<li>
通过属性 <code>mapreduce.framework.name</code> 设置为yarn激活YARN

</ul>
<li>
<strong>Step 2:</strong> 从 <code>resource manager</code> 获取一个新的application id

<li>
client 计算分片

<ul>
<li>
分片也可以在集群中计算

<li>
通过属性 <code>yarn.app.mapreduce.am.compute-splits-in-cluster</code> 激活

</ul>
<li>
<strong>Step 3:</strong> 拷贝application的相关资源到HDFS（jar,configuration,分片结果等），jar文件会有很多份

<li>
<strong>Step 4:</strong> 在 <code>resource manager</code> 调用 submitApplication() 提交一个application

</ol>

<p>
<strong>初始化Job:</strong>
</p>
<ol>
<li>
<strong>Step 5a:</strong> scheduler 分配一个 <code>container</code>

<li>
<strong>Step 5b:</strong> <code>resource manager</code> 在这个 <code>container</code> 上运行 <code>application master</code>, 并在 <code>node manager</code> 的管理下

<li>
<strong>Step 6:</strong> <code>applicaiton master</code> 初始化一个application

<ul>
<li>
<code>applicaiton master</code> 对应类  MRAppMaster

<li>
创建一系列对象追踪job的进度 

<li>
稍后它会从task处得到进度和完成信息

</ul>
<li>
<strong>Step 7:</strong> <code>applicaiton master</code> 从分布式系统中获取计算好的splits

<li>
 <code>application master</code> 创建tasks:

<ul>
<li>
为每一个split创建一个map task

<li>
创建redeuce task, 数量由属性 <code>mapreduce.job.reduces</code> 指定

</ul>
<li>
 如果是一个小的job,<code>application master</code> 将会在它自身的jvm上运行些task,如果判定这是一个小的job :

<ul>
<li>
<code>mapreduce.job.ubertask.maxmaps</code> 默认值10

<li>
<code>mapreduce.job.ubertask.maxreduces</code> 默认值1

<li>
<code>mapreduce.job.ubertask.maxbytes</code> 默认值HDFS Block

<li>
<code>mapreduce.job.ubertask.enable</code> 默认值true

</ul>
</ol>

<p>
<strong>分配Task:</strong>
</p>
<ol>
<li>
<strong>Step 7:</strong> <code>applicaiton master</code> 向 <code>resource manager</code> 申请运行map task和reduce task所需的资源 <code>containers</code>

<li>
为task分配内存,通过下列属性来指定

<ul>
<li>
<code>mapreduce.map.memory.mb</code> map task的内存  默认:1024M

<li>
<code>mapreduce.reduce.memory.mb</code> reduce task的内存   默认:1024M

<li>
<code>yarn.scheduler.capacity.minimum-allocation-mb</code> 最小值限制，默认为1G

<li>
<code>yarn.scheduler.capacity.maximum-allocation-mb</code>  最大值限制，默认为10G

<li>
分配给task的内存必须在最小值和最大值之间，且是最小值的整数倍

<li>
如果不是整数倍，会自动上升为整数倍

</ul>
</ol>

<p>
<strong>运行Task:</strong>
</p>
<ol>
<li>
task被分配了一个资源 <code>container</code> 

<li>
<strong>Step 9a:</strong> <code>application master</code> 通知相应的 <code>node manager</code> 运行这个 <code>container</code>

<li>
<strong>Step 9b:</strong> 通过类YarnChild启动一个新jvm

<li>
<strong>Step 10:</strong> 从分布式缓存中获取运行task所需的资源：jar configuration etc.

<li>
<strong>Step 11:</strong> 运行这个task

</ol>

<p>
<strong>进度与状态报告:</strong>
</p>
<ol>
<li>
task向 <code>application master</code> 汇报进度与状态

<li>
<code>application master</code> 每三秒绘聚一次整个job的概况

<li>
client 每秒向 <code>application master</code> 拉取一次job的进度与状态

<ul>
<li>
可能通过属性 <code>mapreduce.client.progressmonitor.pollinterval</code> 修改这个时间值 

</ul>
</ol>
<p>
<img src="mapreduce_yarn_report.png" />
</p>

<p>
<strong>Job完成:</strong>
</p>
<ol>
<li>
client会向 <code>application master</code> 轮询job是否完成

<ul>
<li>
通过调用Job的方法waitForCompletion()

<li>
间隔时间通过属性 <code>mapreduce.client.completion.pollinterval</code> 指定

</ul>
<li>
一旦完成：

<ol>
<li>
<code>application master</code> 和 <code>containers</code> 执行cleanup操作

<li>
调用OutputCommiter的cleanup方法

<li>
job的信息被job history server归档

</ol>
</ol>

<h2 id="toc_1.2">处理失败</h2>

<h3 id="toc_1.2.1">经典MapReduce(MapReduce 1)的失败</h3>
<p>
<strong>task失败:</strong>
</p>
<ol>
<li>
RuntimeException

<ul>
<li>
退出前通知tasktracker

<li>
将这个task attempt标记为失败

<li>
tasktracker释放一个task slot,等待新的任务

</ul>
<li>
流式task非0退出码

<ul>
<li>
通过属性 <code>stream.non.zero.exit.is.failure</code> 设置

</ul>
<li>
jvm进程突然结束

<ul>
<li>
tasktracker发现task进程结束

<li>
将这次task attemp标记为失败

</ul>
<li>
task长时间挂起

<ul>
<li>
tasktracker在一定时间内收不到进度报告，标记为失败

<li>
通过属性 <code>mepred.task.timeout</code> 设置超时时间

<li>
0这关掉该特性，永不超时

</ul>
<li>
jobtracker 发现一个 task attempt 失败

<ul>
<li>
重新高度这个task

<li>
会尽量避免使用这个task上次失败所在的tasktracker

<li>
同一个task默认失败4次，就不会在重试

<li>
可以通过属性 <code>mapred.map.max.attempts</code> 和 <code>mapred.reduce.max.attempts</code> 设置

</ul>
<li>
task失败达到一定比例后，才会标记一个job的失败,job退出执行

<ul>
<li>
通过属性 <code>mapred.max.map.failures.percent</code> 和 <code>mapred.max.reduce.failures.percent</code> 设置

</ul>
</ol>

<p>
<strong>tasktracker失败:</strong>
</p>
<ol>
<li>
tasktracker失败，就不会向jobtracker发送心跳

<li>
jobtracker长时间收不到tasktracker的心跳，就会标记tasktracker失败

<ul>
<li>
通过属性设置这个时间 <code>mapred.tasktracker.expiry.interval</code> 默认为10分钟

</ul>
<li>
jobtracker会将该tasktracker从池中移出，不再参与task的分配

<li>
在那个tasktracker上所有未完成job的所有task都会重新执行，因为那个tasktracker的所在机器很可能不能访问了

<li>
tasktracker 黑名单

<ol>
<li>
同一个job的task失败数超过4个，就会标记该tasktracker失败，通过属性 <code>mapred.max.tracker.failures</code> 设置 

<li>
一个tasktracker失败超过4次，就会加入黑名单, 通过属性 <code>mapred.max.tracker.blacklists</code> 设置 

<li>
黑名单中的tasktracker不参与task的分配

<li>
黑名单有过期时间，为1天

</ol>
</ol>


<p>
<strong>jobtracker失败:</strong>
</p>
<ol>
<li>
这是很严重的错误，单点错误，没有有效的应对方案

<li>
一个失败的jobtracker重启后，会尝试恢复相关的job

<ol>
<li>
通过属性 <code>mapred.jobtracker.restart.recover</code> 开启这项功能，默认是关闭

<li>
但这项功能并不可靠，不见意使用

</ol>
</ol>

<h3 id="toc_1.2.2">YARN(MapReduce 2)中的失败</h3>
<p>
<strong>task失败:</strong>
</p>
<ol>
<li>
RuntimeException

<ul>
<li>
退出前通知 <code>application master</code> 

<li>
将这个task attempt标记为失败

</ul>
<li>
流式task非0退出码

<ul>
<li>
通过属性 <code>stream.non.zero.exit.is.failure</code> 设置

</ul>
<li>
jvm进程突然结束

<ul>
<li>
<code>application master</code> 发现task进程结束

<li>
将这次task attemp标记为失败

</ul>
<li>
task长时间挂起

<ul>
<li>
<code>application master</code> 在一定时间内收不到进度报告，标记为失败

<li>
通过属性 <code>mepred.task.timeout</code> 设置超时时间

<li>
0这关掉该特性，永不超时

</ul>
<li>
如何确定一个task的失败

<ul>
<li>
重试4次,还不成功

<li>
可以通过属性 <code>mapred.map.max.attempts</code> 和 <code>mapred.reduce.max.attempts</code> 设置

</ul>
<li>
task失败达到一定比例后，才会标记一个job的失败,job退出执行

<ul>
<li>
通过属性 <code>mapred.max.map.failures.percent</code> 和 <code>mapred.max.reduce.failures.percent</code> 设置

</ul>
</ol>

<p>
<strong>application master失败:</strong>
</p>
<ol>
<li>
实际上就是job的失败

<li>
job可以有重试一次，默认是一次

<ul>
<li>
可以通过属性 <code>yarn.resourcemanager.am.max-retries</code> 修改

</ul>
<li>
重试

<ol>
<li>
resource manager会分配一个新的container运行一个新的applicaiton master

<li>
application master会重新运行所有的task

<li>
但application master可以只运行那些还未成功的task

<li>
通过属性 <code>yarn.app.mapreduce.am.job.recovery.enable</code> 开启这个功能

<li>
client 会经历一个timeout后，从resource manager获取新 application master 的地址

</ol>
</ol>

<p>
<strong>node manager失败:</strong>
</p>
<ol>
<li>
node manager 失败

<li>
就不会向resource manager发送心跳

<li>
resouce manager长时间收不到心跳

<ul>
<li>
通过属性 <code>yarn.resourcemanager.ns.liveness-monitor.expiry-interval-ms</code> 设置

</ul>
<li>
resouce manager就会将结点从结点列表中去除

<li>
而在该结点上所有的task和application master都会根据前面的规则进行recovery

<li>
node manager黑名单

<ul>
<li>
超过三个task失败

<li>
通过属性 <code>mapreduce.job.maxtaskfailures.per.tracker</code> 设置

</ul>
</ol>
   
<p>
<strong>resource manager失败:</strong>
</p>
<ol>
<li>
resouce manager被设计成可以持久化保持状态

<li>
resouce manager失败后

<li>
resouce manager会重启并恢复状态

<li>
状态信息由node managers和正在运行的job组成

</ol>

<h2 id="toc_1.3">Job的调度</h2>
<p>
默认是FIFO，只能串行
</p>

<h3 id="toc_1.3.1">公平调度</h3>
<ol>
<li>
每个用户获得相同的资源

</ol>

<h3 id="toc_1.3.2">容量调度</h3>
<ol>
<li>
按容量分成不同的queue

<li>
每个queue在fifo

</ol>

<h2 id="toc_1.4">Shuffle和排序</h2>
<ol>
<li>
MapReduce保证到达reducer的输入是按key排好序的

<li>
排序并将map的输出作为输入传给reducer的这个处理过程称之为 <code>shuffle</code>

</ol>
 
<h3 id="toc_1.4.1">Map阶段</h3>
<p>
<img src="mapreduce_shuffle_mapside.png" />
</p>
<ol>
<li>
mapper会将输出写入一段缓存，并进行一些预排序

<li>
mapper默认拥有一个默认100m的缓存

<ul>
<li>
可能通过属性 <code>io.sort.mb</code> 调节

</ul>
<li>
当缓存中的大小到达一个阀值，就会启动一个后台线程，将数据写入磁盘

<ul>
<li>
这个阀值通过属性 <code>io.sort.spill.percent</code> 控制 

<li>
这时mapper的输出还是直接写入缓存，后台线程再将数据从缓存写入磁盘

<li>
如果缓存满了，mapper就会阻塞

<li>
磁盘目录由属性 <code>mapred.local.dir</code> 指定

</ul>
<li>
写入磁盘前，会将数据根据它们最终的去向(reducer)进行分组(partition)

<ul>
<li>
针对每一组，有一后台线程在内存中就会对它根据key排序

<li>
如果指定了combiner function,就会对这些排好序的数据运行

<li>
将会得到更紧凑的数据，更少的数据写入磁盘,更少的数据传到reducer

</ul>
<li>
每当缓存到达阀值，都会产生一个新的spill文件

<li>
mapper结束时，就会得到多个spill文件

<li>
这些文件会合并成一个分组且排好序的文件

<ul>
<li>
属性 <code>io.sort.factor</code> 控制最大有多少流用于merge操作，默认为10

</ul>
<li>
如果已经有至少3个spill文件，输出写入前，对其再次进行combiner操作

<ul>
<li>
属性 <code>min.num.spills.for.combiner</code>

</ul>
<li>
对mapper输出启用压缩是一个不错的主意

<ul>
<li>
<code>mapred.compress.map.output</code> 设置为true启用压缩

<li>
<code>mapred.map.output.compression.codec</code> 指定压缩算法

</ul>
</ol>

<h3 id="toc_1.4.2">Reduce阶段</h3>
<ol>
<li>
Reducer需要将自己所要的分组数据从多个mapper处拷过来

<li>
Reducer有一组copy线程来做这个，默认为5个线程

<ul>
<li>
可以通过属性 <code>mapred.reduce.parallel.copies</code> 调节

</ul>
<li>
Reducer如何知道从哪些机器上获取数据

<ul>
<li>
Mapper工作时会通知tasktracker,tasktracker再通知jobtracker(yarn中是application master)

<li>
针对一个job, jobtracker(yarn中是application master)会知道这些信息

<li>
reducer会不停的询问这些信息，直到获取所有的数据

</ul>
<li>
从Mapper过来的数据会先放到内存中的一个缓冲区中

<ul>
<li>
缓冲区大小由 <code>mapred.job.shuffle.input.buffer.percent</code> 指定内存堆中多少比例用作缓冲区

</ul>
<li>
缓冲区到达一个阀值，就会merge并写到磁盘上

<ul>
<li>
<code>mapred.job.shuffle.merge.percent</code> 缓冲区的大小到达一个阀值 

<li>
<code>mapred.inmem.merge.threthold</code> map input的个数到达一个阀值

<li>
merge并写入磁盘时会运行combiner,如果存在的话

</ul>
<li>
随着磁盘上的文件增多，一个后台线程会不停的将这些文件合并排序成更大的文件，为最终的merge做优化

<li>
如果mapper过来的数据是压缩过的，内存中就会进行解压

<li>
所有mapper的数据都过来后，进行最终的merge

<ul>
<li>
这个过程是循环来做的

<li>
每次只将指定数目的文件merge成一个, 数目通过属性 <code>io.sort.factor</code> 指定, 默认为10

<li>
但最后一轮merge并不会写入磁盘，而是将内存与磁盘数据进行merge并传给reducer

</ul>
<li>
针对合并已排序的数据，reducer function会依次对每个key调用

<li>
输出写入文件系统，通常是hdfs

</ol>

<h3 id="toc_1.4.3">配置调整</h3>

<h2 id="toc_1.5">Task的运行</h2>

<h3 id="toc_1.5.1">Speculative Task 推测式任务</h3>
<ol>
<li>
mapreduce是通过将一个job分解成很多并行task来缩短整个job的时间

<li>
但这些并行task中有一个和几个由于不明原因运行慢就会加大整个job的时间

<li>
mapreduce会推测识别出这些task，并另外启动一个task，谁先完成就用谁，另一个杀掉

<li>
推测式任务策略并不是为每一个任务建立一个镜像任务来实现的，这将是极大的资源浪费

<li>
推测式任务策略是在所有任务都开始一段时间后，某个任务进度明显小于所有任务平均进度时，就会为它建立一个任务，做为备选。

</ol>


<h3 id="toc_1.5.2">跳过不合法记录</h3>
<ol>
<li>
通过属性 `` 开启此功能

<li>
如果一个task因为一条记录失败两次，第三次尝试时，就会跳过该记录

<li>
所以默认的情况下，一个task只能跳过一条记录（默认最大尝试次数为4）

<li>
可以调大task的尝试次数，来应付更多的不合法记录

</ol>

			</div>

			<div id="bottom">
				&copy; 2012 王兴朝
			</div>
		</div>
	<div>
</body>
</html>
