<!DOCTYPE html>
<html>
<head>

<title>Hadoop File System</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">

<script type="text/javascript" src="../../js/jquery-1.6.4.min.js"></script>

<link rel="Stylesheet" type="text/css" href="../../js/sh/styles/shCore.css">
<link rel="Stylesheet" type="text/css" href="../../js/sh/styles/shThemeRDark.css">
<script type="text/javascript" src="../../js/sh/scripts/shCore.js"></script>
<script type="text/javascript" src="../../js/sh/scripts/shAutoloader.js"></script>

<script type="text/javascript" src="../../js/main.js"></script>
<script type="text/javascript" src="../../js/ASCIIMathML.js"></script>

<link rel="Stylesheet" type="text/css" href="../../style.css">
<link rel="Stylesheet" type="text/css" href="../../css/main.css">

</head>
<body>
	<div class="hidden">
		<input id="root_path" type="text" value="../../">
	</div>
	<div id="body-wrapper">
		<div id="container">
			<div id="top">
				<div id="page-title">
					<a href="../../index.html">烂笔头</a>
				</div>
				<ul id="top-nav">
				</ul>
			</div>
			<div id="middle">
				
<div class="toc">
<ul>
<li><a href="#toc_1">Hadoop File System</a>
<ul>
<li><a href="#toc_1.1">HDFS的设计 </a>
<li><a href="#toc_1.2">HDFS</a>
<ul>
<li><a href="#toc_1.2.1">数据块 </a>
<li><a href="#toc_1.2.2">namenode &amp; datanode</a>
<li><a href="#toc_1.2.3">命令行接口 </a>
<li><a href="#toc_1.2.4">基本文件系统操作</a>
<li><a href="#toc_1.2.5">Hadoop文件系统</a>
<li><a href="#toc_1.2.6">接口</a>
<li><a href="#toc_1.2.7">JAVA接口</a>
</ul>
<li><a href="#toc_1.3">数据流 </a>
<ul>
<li><a href="#toc_1.3.1">文件读取剖析</a>
<li><a href="#toc_1.3.2">如何衡量节点的距离 </a>
<li><a href="#toc_1.3.3">文件写入分析</a>
<li><a href="#toc_1.3.4">如何确定复本布局和pipeline</a>
<li><a href="#toc_1.3.5">一致性 Coherency Model</a>
</ul>
<li><a href="#toc_1.4">通过distcp进行复制</a>
<li><a href="#toc_1.5">hadoop存档 Hadoop Archives</a>
<ul>
<li><a href="#toc_1.5.1">hadoop存档工具</a>
</ul>
</ul>
</ul>
</div>
<h1 id="toc_1">Hadoop File System</h1>
<ul>
<li>
当数据集大小超过一台物理机器，需要对它分区存储到不同的机器上。

<li>
跨机器管理存储的文件系统称之为分布式文件系统，Distributed File System

<li>
HDFS: Hadoop distributed file system.

</ul>

<h2 id="toc_1.1">HDFS的设计 </h2>
<ul>
<li>
<strong>超大文件:</strong> MB GB TB PB 

<li>
<strong>流式数据访问:</strong>

<ul>
<li>
一次写入，多次读入

<li>
每次数据分析涉及数据集中的大部分或全部

</ul>
<li>
<strong>商业硬件:</strong> 并不需要运行在昂贵的可靠的商业硬件上

<li>
<strong>商业硬件:</strong> 并不需要运行在昂贵的可靠的商业硬件上

<li>
<strong>低时间延迟的访问:</strong>

<ul>
<li>
不适合低时间延迟的访问，比如几十毫秒

<li>
为高吞吐量做的优化， 但可能会高时间延迟为代价

</ul>
<li>
<strong>大量小文件:</strong>

<ul>
<li>
元数据存储在Namenode上

<li>
文件总数的大小依赖于Namenode的内存大小 

</ul>
<li>
<strong>不支持多用户写和多用户同时修改:</strong>

</ul>
 
<h2 id="toc_1.2">HDFS</h2>
<h3 id="toc_1.2.1">数据块 </h3>
<ul>
<li>
<strong>磁盘块:</strong> 磁盘进行读写的最小单位, 一般为512字节

<li>
文件系统块是磁盘块的整数倍，一般为几千字节

<li>
HDFS的块默认为64m

<ul>
<li>
小于一个块大小的文件不会占满一个块

<li>
块设的大是为了最小化寻址开销

<li>
块过大也不行，mapreduce一次处理一个块

<li>
块的大小要根据实际情况来调整，比如寻址占传输时间的1/100

</ul>
<li>
使用块的好处

<ul>
<li>
一个文件的大小可以大于任意一个磁盘的大小

<li>
简化了存储子系统，文件的元数据不和块数据存储在一起

<li>
适合备份和容错，任何一个块都有复本。

</ul>
</ul>
<h3 id="toc_1.2.2">namenode &amp; datanode</h3>
<ul>
<li>
管理者 工作者模式

<li>
namenode:

<ul>
<li>
管理者

<li>
管理文件系统的命名空间

<li>
维护文件系统树及树内所的目录与文件

<li>
信息以两个文件永久保存：命名空间镜像文件与编辑日志文件

<li>
记录文件中每个块所在数据结点的信息，但不永久保存，系统启动时重建

</ul>
<li>
datanode:

<ul>
<li>
工作者

<li>
存储校验数据块

<li>
定期向namenode发送数据块列表

</ul>
<li>
失去namenode等于失去文件系统，所namenode容错非常重要：

<ul>
<li>
备份namenode上的元数据，多份同进写

<li>
使用辅助namenode, 定期合并编辑日志生成空间镜像

</ul>
</ul>
<h3 id="toc_1.2.3">命令行接口 </h3>
<h3 id="toc_1.2.4">基本文件系统操作</h3>
<pre  class="brush:bash">
hadoop fs -help
</pre>
  
<h3 id="toc_1.2.5">Hadoop文件系统</h3>
<ul>
<li>
是一个气象的文件系统

<li>
<strong>HDFS只是它的一个实现 </strong>

<li>
抽象类定义了一个文件系统接口：<code>org.apache.hadoop.fs.FileSystem</code>

<li>
还有很多其他实现：Local HDFS HFTP HSFTP HAR hfs FTP S3 

</ul>

<h3 id="toc_1.2.6">接口</h3>
<ul>
<li>
java

<li>
C

<li>
Thrift

<li>
WebDev

<li>
FUSE

<li>


<li>
HDFS两种特定的

<ul>
<li>
HTTP: namenode 50070; datanode 50075

<li>
FTP

</ul>
</ul>
<h3 id="toc_1.2.7">JAVA接口</h3>
<ul>
<li>
<strong>Configuration FileSystem FSDataInpuStream FSDataOutputStream Path FileStatus PathFileter</strong>

<li>
<strong>Configuration</strong>

<ol>
<li>
文件系统的配置

</ol>
<li>
<strong>FileSystem</strong>

<ol>
<li>
代表文件系统

<li>
FileSystem.get(uri, config) :创建

<li>
FSDataInputStream open(path) :获取指定文件的输入流

<li>
FSDataOutputStream create(path) :创建一个文件并获得它的输出流

<li>
FSDataOutputStream append(path) :获取指定文件的输出流，并在其后添加数据

<li>
boolean mkdirs(path) :创建必要但还没有的父目录

<li>
FileStatus getFileStatus(path) : 获取指定文件的文件状态

<li>
FileStatus[] globFileStatus(path) : 获取与文件相匹配的所有文件的文件状态

<li>
FileStatus[] globFileStatus(path， pathFilter) : 获取与文件相匹配的所有文件的文件状态

<li>
FileStatus[] listFileStatus(path) : 用来列目录

<li>
FileStatus[] listFileStatus(path， pathFilter) : 用来列目录

<li>
boolean delete(path, boolean recursive) : 删除文件或目录， 是否递归删除

</ol>
<li>
<strong>FSDataInputStream</strong>

<ol>
<li>
支持随机访问

<li>
seek(long):设置postion

<li>
long getPos():获取postion

<li>
int read(long position, byte[] buffer, int offset, int length)

<li>
void readFully(long position, byte[] buffer, int offset, int length)

<li>
void readFully(byte[] buffer, int offset, int length)

</ol>
<li>
<strong>FSDataOutputStream</strong>

<ol>
<li>
用于写数据 

<li>
支持随机访问

<li>
long getPos():设置postion

<li>
但不允许定位

<li>
只支持对一个打开的文件从头顺序写入， 或从末尾处添加

</ol>
<li>
<strong>FileStatus</strong>

<ol>
<li>
用于查询一个文件状态

</ol>
<li>
<strong>Path</strong>

<ol>
<li>
代表一个文件路径

</ol>
<li>
<strong>PathFileter</strong>

<ol>
<li>
当listStatus时，过滤文件

</ol>
<li>
例子：

<ol>
<li>
读

<li>
写

<li>
目录

<li>
列文件

<li>
删除

</ol>
</ul>
   
<h2 id="toc_1.3">数据流 </h2>
<h3 id="toc_1.3.1">文件读取剖析</h3>
<p>
<img src="reading-data-from-hdfs.png" />
</p>
<ol>
<li>
fileSystem.open打开要读取的文件 

<ul>
<li>
这个文件就是HDFS分布式文件系统中的一个实例

</ul>
<li>
DistributedFileSystem通过RPC调用NameNode确定文件 *起始块* 所在的位置

<ul>
<li>
NameNode返回所有包含该块的DataNode地址

<li>
并按网络距离排序

<li>
比如客户端所在就是一个DataNode, 而该块在此DataNode上刚好存有复本，就会发起本地读取

<li>
返回一个FSDataInputStream对象，供客户端使用. 进而转而封装为HdfsDataInputStream,管理着Namenode和Datanode之间的io

</ul>
<li>
对输入流(FSDataInputStream)调用read()方法

<li>
HdfsDataInputStream会从最近的Datanode读入

<ul>
<li>
不停的read()直到块的末尾

</ul>
<li>
HdfsDataInpuStream会寻找  <strong>下一个块</strong>  的 <strong>最优Datanode</strong> 继续读取

<ul>
<li>
对于 <strong>下一块 </strong> 也需要向Namenode查询所需的Datanode

</ul>
<li>
读取完毕调用FSDataInputStream的close()方法进行关闭

</ol>
 
<p>
<strong>HdfsDataInputStream与Datanode通信出错时：</strong>
</p>
<ol>
<li>
尝试比这个Datanode次邻近的Datanode读取

<li>
它会记住这个出错的Datanode,后续的块也不会从这里读取

<li>
它也会校验读取的数据是否完整,如果发现损坏的块

<li>
如果不完整，再尝试从其他Datanode读取数据之前，通知Namenode

</ol>
<p>
<strong>设计优劣:</strong>
</p>
<ol>
<li>
Namenode告知client最佳的Datanode

<li>
client直接从Datenode取数据

<li>
因为Datanode分散在集群中

<li>
所以很容易扩展到大量并发客户端

<li>
Namenode仅提供块位置请求，而不提供数据请求，很高效

<li>
随着客户端的增加， Namenode还是会成为瓶颈

</ol>

<h3 id="toc_1.3.2">如何衡量节点的距离 </h3>
<p>
<img src="network-distance.png" />
</p>
<ul>
<li>
海量数据处理中，主要的限制因素是节点间的传输速率 -- 带宽

<li>
将带宽作为衡量距离的标准

<li>
带宽是很难衡量比较的

<li>
Hadoop采用了一个简单的方法，以下带宽以次递减

<ol>
<li>
同一节点中的不同进程

<li>
同一机架上的不同节点

<li>
同一数据中心不同机架上的节点

<li>
不同数据中心里的节点

</ol>
<li>
这些信息需要我们辅助定义

</ul>

<h3 id="toc_1.3.3">文件写入分析</h3>
<p>
<img src="writing-data-from-hdfs.png" />
</p>
<ol>
<li>
调用DistributedFileSystem的create()来创建文件

<li>
DistributedFileSystem对namenode发起一个RPC, 在文件系统命名空间中创建一个文件，此时还没有数据块

<ul>
<li>
namenode执行各种检查确保要创建的文件还不存在

<li>
namenode检查客户端的权限

<li>
检查通过为新文件添加一条记录， 否则抛出异常

<li>
返回一个FSDataOutputStream,  里面封装了HdfsOutputStream, 负责datanode与namenode之间的通信

</ul>
<li>
调用write()写入数据

<ul>
<li>
HdfsOutputStream将数据分成一个个数据包，并写入内部队列data queue

<li>
DataStreamer 处理数据队列

<li>
它负责请求namenode分配blocks存储数据，namenode生成一个合适的datanode列表用于存储复本

<li>
这组datanode组成一个管线，其中的数目就是我们所设的备份数replication

</ul>
<li>
DataStreamer将数据包流式传入管线中的第一个节点的datanode

<ul>
<li>
该datanode存储数据并同时把数据发往管线中的下一个datanode

<li>
以此类推，datanode在存储数据的同时发往管线中的下一个datanode(如果存在的话，也就是直到最后一个）

</ul>
<li>
HdfsOutputStream等待datanode的确认返回 ack queue

<ul>
<li>
HdfsOutStream维护了一个内部队列，等待daqanode的确认返回, 称为确认队列ack queue

<li>
只有数据包在管线中的所有datanode收到后，才从ack queue中移除

</ul>
<li>
数据写入完毕,调用close()方法

<ul>
<li>
触发将剩余的数据包写入管线中

<li>
向namenode发送确认完成信号

</ul>
<li>
向namenode发送确认完成信号（在close方法内部）

<ul>
<li>
namenode已知文件由哪些数据块组成

<li>
因DataStreamer曾要求namenode分配数据块

<li>
等待数据块进行最小量的复制

<li>
完成返回

</ul>
</ol>
   
<p>
<strong>在写入过程中，datanode发生故障:</strong> 
</p>
<ol>
<li>
关闭管线 pipeline

<li>
把ack queue中的所有数据包添加到data queue的前端, 确保故障datanode后的datanode不会漏掉数据包

<li>
为正在写的数据块分配一个标识符，并传给namenode;用于在　故障datanode　恢复后，清除这个未写完的数据块。

<li>
将故障datanode从pipeline中移除，将剩余的数据块写入pipeline中剩余的datanode中

<li>
namenode发现复本量不足时，会在另外的节点上创建新的复本，后续的数据块正常处理。（ <strong>这个datanode会加入pipeline吗？</strong> ）

</ol>
 
<p>
<strong>在写入过程中，多datanode同时故障:</strong> 
</p>
<ol>
<li>
很少发生

<li>
只要复本数达到dfs.replication.min(默认为1),就认为写入成功

<li>
事后会通过异步复制，达到dfs.replication(默认为3)

</ol>
 
<h3 id="toc_1.3.4">如何确定复本布局和pipeline</h3>
<p>
<img src="replica-pipeline.png" />
</p>
<ol>
<li>
第1个复本：

<ul>
<li>
client所在的datanode

<li>
若client在集群之外，随机选取一个datanode

</ul>
<li>
第2个复本：

<ul>
<li>
在和第1个datanode <strong>不同</strong> 的机架上随机选一个datanode

</ul>
<li>
第3个复本：

<ul>
<li>
在和第2个datanode <strong>相同</strong> 的机架上随机选一个datanode（第2个datanode除外)

</ul>
<li>
其他复本：

<ul>
<li>
除了前3个datanode外，在cluster中随机选取一个datanode

</ul>
</ol>
  
<ul>
<li>
选取完毕后，这些datanode就组成了一个管线

</ul>
 
<h3 id="toc_1.3.5">一致性 Coherency Model</h3>
<ul>
<li>
Coherency Model:一个文件的读和写操作对其数据的可见性

<li>
HDFS牺牲了一些POSIX的一些要求，所以一些操作可能并不如你期望的一样

</ul>
 
<ol>
<li>
创建一个文件扣，并不能立即可见：
<pre  class="brush:java">
    Path p = new Path("p");
    fs.create(p);
    assertThat(fs.exists(p), is(true));
</pre>

<li>
写入的数据并不保证立即可见，即使已经flush()
<pre  class="brush:java">
    Path p = new Path("p");
    OutputStream out = fs.create(p);
    out.write("content".getBytes("UTF-8"));
    out.flush();
    assertThat(fs.getFileStatus(p).getLen(), is(0L));//为0
</pre>

<li>
当写满一个块后就可见了，也就是只有正在写的block是不可见的

<li>
提供了一个方法来和datanodes同步，保证数据的可见性，FSDataOutputStream上的sync()
<pre  class="brush:java">
        Path p = new Path("p");
        FSDataOutputStream out = fs.create(p);
        out.write("content".getBytes("UTF-8"));
        out.flush();
        out.sync();
        assertThat(fs.getFileStatus(p).getLen(), is(((long) "content".length())));
</pre>

<li>
关闭一个文件会隐匿的调用close()
<pre  class="brush:java">
        Path p = new Path("p");
        OutputStream out = fs.create(p);
        out.write("content".getBytes("UTF-8"));
        out.close();
        assertThat(fs.getFileStatus(p).getLen(), is(((long) "content".length())));
</pre>

</ol>
    
<h2 id="toc_1.4">通过distcp进行复制</h2>
<ul>
<li>
可以从hadoop中复制大量数据

<li>
也可将大量数据复制到hadoop

<li>
跨集群复制
<pre  class="brush:bash">
  hadoop distcp hdfs://namenode1/foo hdfs://namenode2/bar
  # 将foo目录复制到bar目录下，如果bar不存在，会自动创建
  # 可以将多个源路径复制到一个目标路径下
  # 源路径必须是绝对路径
   
  hadoop distcp --update hdfs://namenode1/foo hdfs://namenode2/bar/foo
  # 将一个集群中foo目录下的内容， 复制到另一个集群中的/bar/foo下
  # 只复制更新过的文件
  # 注意对路径的解释变了
   
  hadoop distcp --overwrite hdfs://namenode1/foo hdfs://namenode2/bar/foo 
  # 对路径的解释和--update一致
  
</pre>

</ul>
    
<ul>
<li>
默认每个节点最多分配20个map任务

<li>
如果是两个hdfs之间的复制，需要他们版本一致

<li>
<strong>保持hdfs集群的均衡性cluster balanced</strong>
<pre  class="brush:bash">
  hadoop distcp -m 1 hdfs://namenode1/foo hdfs://namenode2/bar
  #只用一个map进行复制，丢失均衡性
</pre>

</ul>
    
<h2 id="toc_1.5">hadoop存档 Hadoop Archives</h2>
<p>
问题：
</p>
<ul>
<li>
hadoop存小文件很低效

<li>
因为大量的小文件会耗光namenode的内存

<li>
存小文件并不会带空间的多占用，一m的文件就是1m，而不会变成一个block，64m or 128m

</ul>

<h3 id="toc_1.5.1">hadoop存档工具</h3>
<ul>
<li>
hadoop存档文件是存档工具根据一组成文件创建的

<li>
是一个MapReduce任务

<li>
需要一个MapReduce集群 

</ul>

<ol>
<li>
创建一个存档文件
<pre  class="brush:bash">
  hadoop archive -archiveName files.har /my/files /my
  # 可以有多个源文件目录
  # 最后一个目录是归档文件存储的目录
   

    hadoop fs -ls /my/files.har
    
  # 使用har风格的url和har文件进行交互
    hadoop fs -lsr har:///my/files.har
    hadoop fs -lsr har:///my/files.har/my/files/dir
    hadoop fs -lsr har://hdfs-localhost:8020/my/files.har/my/files/dir
    
   # 删除一个har文件
    hadoop fs -rmr /my/files.har
</pre>

</ol>

			</div>

			<div id="bottom">
				&copy; 2012 王兴朝
			</div>
		</div>
	<div>
</body>
</html>
