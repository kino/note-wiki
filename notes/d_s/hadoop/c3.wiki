%title Hadoop File System
%toc
=Hadoop File System=
* 当数据集大小超过一台物理机器，需要对它分区存储到不同的机器上。
* 跨机器管理存储的文件系统称之为分布式文件系统，Distributed File System
* HDFS: Hadoop distributed file system.

==HDFS的设计 ==
* *超大文件:* MB GB TB PB 
* *流式数据访问:*
  * 一次写入，多次读入
  * 每次数据分析涉及数据集中的大部分或全部
* *商业硬件:* 并不需要运行在昂贵的可靠的商业硬件上
* *商业硬件:* 并不需要运行在昂贵的可靠的商业硬件上
* *低时间延迟的访问:*
  * 不适合低时间延迟的访问，比如几十毫秒
  * 为高吞吐量做的优化， 但可能会高时间延迟为代价
* *大量小文件:*
  * 元数据存储在Namenode上
  * 文件总数的大小依赖于Namenode的内存大小 
* *不支持多用户写和多用户同时修改:*
 
==HDFS ==
===数据块 ===
* *磁盘块:* 磁盘进行读写的最小单位, 一般为512字节
* 文件系统块是磁盘块的整数倍，一般为几千字节
* HDFS的块默认为64m
  * 小于一个块大小的文件不会占满一个块
  * 块设的大是为了最小化寻址开销
  * 块过大也不行，mapreduce一次处理一个块
  * 块的大小要根据实际情况来调整，比如寻址占传输时间的1/100
* 使用块的好处
  * 一个文件的大小可以大于任意一个磁盘的大小
  * 简化了存储子系统，文件的元数据不和块数据存储在一起
  * 适合备份和容错，任何一个块都有复本。
===namenode & datanode ===
* 管理者 工作者模式
* namenode:
  * 管理者
  * 管理文件系统的命名空间
  * 维护文件系统树及树内所的目录与文件
  * 信息以两个文件永久保存：命名空间镜像文件与编辑日志文件
  * 记录文件中每个块所在数据结点的信息，但不永久保存，系统启动时重建
* datanode:
  * 工作者
  * 存储校验数据块
  * 定期向namenode发送数据块列表
* 失去namenode等于失去文件系统，所namenode容错非常重要：
  * 备份namenode上的元数据，多份同进写
  * 使用辅助namenode, 定期合并编辑日志生成空间镜像
===命令行接口 ===
===基本文件系统操作 ===
{{{ class="brush:bash"
hadoop fs -help
  }}}
  
===Hadoop文件系统 ===
* 是一个气象的文件系统
* *HDFS只是它的一个实现 *
* 抽象类定义了一个文件系统接口：`org.apache.hadoop.fs.FileSystem`
* 还有很多其他实现：Local HDFS HFTP HSFTP HAR hfs FTP S3 

===接口 ===
* java
* C
* Thrift
* WebDev
* FUSE
* 
* HDFS两种特定的
  * HTTP: namenode 50070; datanode 50075
  * FTP
===JAVA接口 ===
* *Configuration FileSystem FSDataInpuStream FSDataOutputStream Path FileStatus PathFileter*
* *Configuration*
  # 文件系统的配置
* *FileSystem*
  # 代表文件系统
  # FileSystem.get(uri, config) :创建
  # FSDataInputStream open(path) :获取指定文件的输入流
  # FSDataOutputStream create(path) :创建一个文件并获得它的输出流
  # FSDataOutputStream append(path) :获取指定文件的输出流，并在其后添加数据
  # boolean mkdirs(path) :创建必要但还没有的父目录
  # FileStatus getFileStatus(path) : 获取指定文件的文件状态
  # FileStatus[] globFileStatus(path) : 获取与文件相匹配的所有文件的文件状态
  # FileStatus[] globFileStatus(path， pathFilter) : 获取与文件相匹配的所有文件的文件状态
  # FileStatus[] listFileStatus(path) : 用来列目录
  # FileStatus[] listFileStatus(path， pathFilter) : 用来列目录
  # boolean delete(path, boolean recursive) : 删除文件或目录， 是否递归删除
* *FSDataInputStream*
  # 支持随机访问
  # seek(long):设置postion
  # long getPos():获取postion
  # int read(long position, byte[] buffer, int offset, int length)
  # void readFully(long position, byte[] buffer, int offset, int length)
  # void readFully(byte[] buffer, int offset, int length)
* *FSDataOutputStream*
  # 用于写数据 
  # 支持随机访问
  # long getPos():设置postion
  # 但不允许定位
  # 只支持对一个打开的文件从头顺序写入， 或从末尾处添加
* *FileStatus*
  # 用于查询一个文件状态
* *Path*
  # 代表一个文件路径
* *PathFileter*
  # 当listStatus时，过滤文件
* 例子：
  # 读
  # 写
  # 目录
  # 列文件
  # 删除
   
==数据流 ==
===文件读取剖析 ===
{{reading-data-from-hdfs.png}}
# fileSystem.open打开要读取的文件 
  * 这个文件就是HDFS分布式文件系统中的一个实例
# DistributedFileSystem通过RPC调用NameNode确定文件 *起始块* 所在的位置
  * NameNode返回所有包含该块的DataNode地址
  * 并按网络距离排序
  * 比如客户端所在就是一个DataNode, 而该块在此DataNode上刚好存有复本，就会发起本地读取
  * 返回一个FSDataInputStream对象，供客户端使用. 进而转而封装为HdfsDataInputStream,管理着Namenode和Datanode之间的io
# 对输入流(FSDataInputStream)调用read()方法
# HdfsDataInputStream会从最近的Datanode读入
  * 不停的read()直到块的末尾
# HdfsDataInpuStream会寻找  *下一个块*  的 *最优Datanode* 继续读取
  * 对于 *下一块 * 也需要向Namenode查询所需的Datanode
# 读取完毕调用FSDataInputStream的close()方法进行关闭
 
*HdfsDataInputStream与Datanode通信出错时：*
  # 尝试比这个Datanode次邻近的Datanode读取
  # 它会记住这个出错的Datanode,后续的块也不会从这里读取
  # 它也会校验读取的数据是否完整,如果发现损坏的块
  # 如果不完整，再尝试从其他Datanode读取数据之前，通知Namenode
*设计优劣:*
  # Namenode告知client最佳的Datanode
  # client直接从Datenode取数据
  # 因为Datanode分散在集群中
  # 所以很容易扩展到大量并发客户端
  # Namenode仅提供块位置请求，而不提供数据请求，很高效
  # 随着客户端的增加， Namenode还是会成为瓶颈

===如何衡量节点的距离 ===
{{network-distance.png}}
* 海量数据处理中，主要的限制因素是节点间的传输速率 -- 带宽
* 将带宽作为衡量距离的标准
* 带宽是很难衡量比较的
* Hadoop采用了一个简单的方法，以下带宽以次递减
  # 同一节点中的不同进程
  # 同一机架上的不同节点
  # 同一数据中心不同机架上的节点
  # 不同数据中心里的节点
* 这些信息需要我们辅助定义

===文件写入分析 ===
{{writing-data-from-hdfs.png}}
# 调用DistributedFileSystem的create()来创建文件
# DistributedFileSystem对namenode发起一个RPC, 在文件系统命名空间中创建一个文件，此时还没有数据块
  * namenode执行各种检查确保要创建的文件还不存在
  * namenode检查客户端的权限
  * 检查通过为新文件添加一条记录， 否则抛出异常
  * 返回一个FSDataOutputStream,  里面封装了HdfsOutputStream, 负责datanode与namenode之间的通信
# 调用write()写入数据
  * HdfsOutputStream将数据分成一个个数据包，并写入内部队列data queue
  * DataStreamer 处理数据队列
  * 它负责请求namenode分配blocks存储数据，namenode生成一个合适的datanode列表用于存储复本
  * 这组datanode组成一个管线，其中的数目就是我们所设的备份数replication
# DataStreamer将数据包流式传入管线中的第一个节点的datanode
  * 该datanode存储数据并同时把数据发往管线中的下一个datanode
  * 以此类推，datanode在存储数据的同时发往管线中的下一个datanode(如果存在的话，也就是直到最后一个）
# HdfsOutputStream等待datanode的确认返回 ack queue
  * HdfsOutStream维护了一个内部队列，等待daqanode的确认返回, 称为确认队列ack queue
  * 只有数据包在管线中的所有datanode收到后，才从ack queue中移除
# 数据写入完毕,调用close()方法
  * 触发将剩余的数据包写入管线中
  * 向namenode发送确认完成信号
# 向namenode发送确认完成信号（在close方法内部）
  * namenode已知文件由哪些数据块组成
  * 因DataStreamer曾要求namenode分配数据块
  * 等待数据块进行最小量的复制
  * 完成返回
   
*在写入过程中，datanode发生故障:* 
# 关闭管线 pipeline
# 把ack queue中的所有数据包添加到data queue的前端, 确保故障datanode后的datanode不会漏掉数据包
# 为正在写的数据块分配一个标识符，并传给namenode;用于在　故障datanode　恢复后，清除这个未写完的数据块。
# 将故障datanode从pipeline中移除，将剩余的数据块写入pipeline中剩余的datanode中
# namenode发现复本量不足时，会在另外的节点上创建新的复本，后续的数据块正常处理。（ *这个datanode会加入pipeline吗？* ）
 
*在写入过程中，多datanode同时故障:* 
# 很少发生
# 只要复本数达到dfs.replication.min(默认为1),就认为写入成功
# 事后会通过异步复制，达到dfs.replication(默认为3)
 
===如何确定复本布局和pipeline ===
{{replica-pipeline.png}}
# 第1个复本：
  * client所在的datanode
  * 若client在集群之外，随机选取一个datanode
# 第2个复本：
  * 在和第1个datanode *不同* 的机架上随机选一个datanode
# 第3个复本：
  * 在和第2个datanode *相同* 的机架上随机选一个datanode（第2个datanode除外)
# 其他复本：
  * 除了前3个datanode外，在cluster中随机选取一个datanode
  
* 选取完毕后，这些datanode就组成了一个管线
 
===一致性 Coherency Model ===
* Coherency Model:一个文件的读和写操作对其数据的可见性
* HDFS牺牲了一些POSIX的一些要求，所以一些操作可能并不如你期望的一样
 
# 创建一个文件扣，并不能立即可见：
  {{{ class="brush:java"
    Path p = new Path("p");
    fs.create(p);
    assertThat(fs.exists(p), is(true));
    }}}
# 写入的数据并不保证立即可见，即使已经flush()
  {{{ class="brush:java"
    Path p = new Path("p");
    OutputStream out = fs.create(p);
    out.write("content".getBytes("UTF-8"));
    out.flush();
    assertThat(fs.getFileStatus(p).getLen(), is(0L));//为0
    }}}
# 当写满一个块后就可见了，也就是只有正在写的block是不可见的
# 提供了一个方法来和datanodes同步，保证数据的可见性，FSDataOutputStream上的sync()
  {{{ class="brush:java"
        Path p = new Path("p");
        FSDataOutputStream out = fs.create(p);
        out.write("content".getBytes("UTF-8"));
        out.flush();
        out.sync();
        assertThat(fs.getFileStatus(p).getLen(), is(((long) "content".length())));
    }}}
# 关闭一个文件会隐匿的调用close()
  {{{ class="brush:java"
        Path p = new Path("p");
        OutputStream out = fs.create(p);
        out.write("content".getBytes("UTF-8"));
        out.close();
        assertThat(fs.getFileStatus(p).getLen(), is(((long) "content".length())));
    }}}
    
==通过distcp进行复制 ==
* 可以从hadoop中复制大量数据
* 也可将大量数据复制到hadoop
* 跨集群复制
  {{{ class="brush:bash"
  hadoop distcp hdfs://namenode1/foo hdfs://namenode2/bar
  # 将foo目录复制到bar目录下，如果bar不存在，会自动创建
  # 可以将多个源路径复制到一个目标路径下
  # 源路径必须是绝对路径
   
  hadoop distcp --update hdfs://namenode1/foo hdfs://namenode2/bar/foo
  # 将一个集群中foo目录下的内容， 复制到另一个集群中的/bar/foo下
  # 只复制更新过的文件
  # 注意对路径的解释变了
   
  hadoop distcp --overwrite hdfs://namenode1/foo hdfs://namenode2/bar/foo 
  # 对路径的解释和--update一致
  
    }}}
    
* 默认每个节点最多分配20个map任务
* 如果是两个hdfs之间的复制，需要他们版本一致
* *保持hdfs集群的均衡性cluster balanced*
  {{{ class="brush:bash"
  hadoop distcp -m 1 hdfs://namenode1/foo hdfs://namenode2/bar
  #只用一个map进行复制，丢失均衡性
    }}}
    
==hadoop存档 Hadoop Archives ==
问题：
* hadoop存小文件很低效
* 因为大量的小文件会耗光namenode的内存
* 存小文件并不会带空间的多占用，一m的文件就是1m，而不会变成一个block，64m or 128m

===hadoop存档工具 ===
* hadoop存档文件是存档工具根据一组成文件创建的
* 是一个MapReduce任务
* 需要一个MapReduce集群 

# 创建一个存档文件
  {{{ class="brush:bash"
  hadoop archive -archiveName files.har /my/files /my
  # 可以有多个源文件目录
  # 最后一个目录是归档文件存储的目录
   

    hadoop fs -ls /my/files.har
    
  # 使用har风格的url和har文件进行交互
    hadoop fs -lsr har:///my/files.har
    hadoop fs -lsr har:///my/files.har/my/files/dir
    hadoop fs -lsr har://hdfs-localhost:8020/my/files.har/my/files/dir
    
   # 删除一个har文件
    hadoop fs -rmr /my/files.har
    }}}
*不足 *
* 创建一个存档文件，会创建原始文件的一个复本
* 所以需要和原始文件一样大小的空间
* 一旦创建，不能修改，不能从中删除或添加文件
* har文件虽然可以作为mapreduce的输入，但其不知已经归档，等于修理har中的很多小文件，仍然是低效的

