%title Hadoop I/O
%toc
=Hadoop I/O=

==数据完整性 Integrity ==
为了保证数据不丢失和损坏，在写入或读出数据时进行校验

===HDFS中的数据完整性 ===
*校验和checksum:*
* HDFS的写入数据时计算较验和，在读取时验证校验和
* HDFS对每io.bytes.per.checksum个（default 512)字节计算checksum
* 使用CRC-32, checksum为4bytes
* 默认存储checksum的额外开销低于1%
 
*写入时:*
# Datanode在存储它接收到的data和它们的checksum前，进行验证
# 这些数据来自client或其他datanode
# 当client向pipeline写入数据时:
  * 只有最后一个datanode负责验证checksum
  * 验证出错时，client会收到一个ChecksumException(IOExcpetion的一个子类)
  * 由应用程序来处理这个异常，比如重试
   
*读取时:*
# client读取数据也要进行checksum的验证
# 重新计算checksum并和datanode上存储的checksum进行比较
# datanode还存储了一个checksum的验证日志，这存储最后一次进行checksum验证的时间
# client验证checksum成功后，就会更新这个日志
# 保留这份统计信息对检测损坏磁盘很有价值

*datanode对checksum进行定期验证:*
# datanode使用 *DataBlockScanner*
# 对存储在其上的所有的Block进行定期验证
# 可以有效防止由于bit rot造成的物理磁盘损坏

*修复损坏的block:*
# 可以通过复制一个正常的block产生一个新的正常的block复本
# 当client检测到一个checksum  error时
# 在抛出异常前，client会向namenode报告出现error的block和其所在datanode
# namenode并不会指挥client去修复这个error
# namenode会异步的计划在另外的datanode重建一个这个block的复本，使其复本数量达到设置的水平

*关闭checksum验证:*
* 使用open()读取一个文件前调用FileSystem的setVerifyChecksum(false)方法
* 在shell中使用-get和-copyToLocal时添加-ignoreCrc选项
  * 可以将损坏的文件拷到本地进行检查，很有用的

===LocalFileSystem类 ===

Hadoop中的LocalFileSystem类负责执行客户端的文件验证
# 当创建一个 `filename` 的文件时
# 客户端会在同一目录下同时创建一个隐藏文件 `.filename.crc`, 这种行为是透明的
  * 这个文件包含了对于文件 `filename` 每一个chunk的checksum
  * chunk的大小同样由io.bytes.per.checksum指定
  * 这个文件还同时存储了这次验证所使用的chunk大小(作为元数据），即当前io.bytes.per.checksum的大小
  * 因此以后即使修改了io.bytes.per.checksum的配置，也不影响恢复文件
# 客户端在读取文件时，会验证Checksums, 如果发生error, LocalFileSystem会抛出一个ChecksumException
 
*禁止验证* 

可以禁止验证，特别是当底层系统已经支持的情况下，这时可以 *使用RawLocalFileSystem代替LocalFilesystem*
* 在应用全局内生效：将属性fs.file.impl设置为org.apache.hadoop.fs.RawLocalFileSysem, 它实现了对文件URI的重新映射
* 某次读取使用，可以生成一个RawLocalFileSystem实例
  {{{ class="brush:java"
      Configuration conf = ...
      FileSystem fs = new RawLocalFileSystem();
      fs.initialize(null, conf);
  }}}
  
===ChecksumFileSystem类 ===

LocalFileSystem使用ChecksumFileSystem去做这它的工作（晕死，有完没完)

ChecksumFileSystem很容易给其他的文件系统添加文件验证的功能，它其实是FileSystem类的一个包装类Wrapper
  {{{ class="brush:java"
    FileSystem rawFs = ...
    FileSystem checksummedFs = new ChecksumFileSystem(rawFs);
  }}}
  
# rawFs是具体的底层文件系统
# ChecksumFileSystem的getRawFileSystem()可以获取它
# ChecksumFileSysem的getChecksumFile()可以获得任一个文件校验文件checksumfile的路径
# 发生错误时，会调用ChecksumFileSysem的reportCheckFailure(), 默认什么也不做
  * LocalFileSystem对这个方法的实现 ：
  * 将出错文件 和它的校验文件移到同一设备的一个side directory , 名叫bad_files
  * 管理员应该定期检查这个文件夹
  
==文件压缩File Compression==
*文件压缩的两大好处:*
* 减少存储空间
* 加快传输
 
*常用压缩格式:*
| Compression format | Tool  | Algorithm | Filename extension | Splittable?                       |
|--------------------|-------|-----------|--------------------|-----------------------------------|
| DEFLATE            | N/A   | DEFLATE   | .deflate           | No                                |
| gzip               | gzip  | DEFLATE   | .gz                | No                                |
| bzip2              | bzip2 | bzip2     | .bz2               | Yes                               |
| LZO                | lzop  | LZO       | .lzo               | No(可以预建切分索引，以支持切分） |
| LZ4                | N/A   | LZ4       | .lz4               | No                                |
| Snappy             | N/A   | Snappy    | .snappy            | No                                |

===Codecs===
* 一个codec代表一种压缩解压算法 
* 一个codec是接口CompressionCodec的一个实现
| Compression format | Hadoop CompressionCodec                    |
|--------------------|--------------------------------------------|
| DEFLATE            | org.apache.hadoop.io.compress.DefaultCodec |
| gzip               | org.apache.hadoop.io.compress.GzipCodec    |
| bzip2              | org.apache.hadoop.io.compress.BZip2Codec   |
| LZO                | com.hadoop.compression.lzo.LzopCodec       |
| LZ4                | org.apache.hadoop.io.compress.Lz4Codec     |
| Snappy             | org.apache.hadoop.io.compress.SnappyCodec  |

*使用CompressionCodec对流进行压缩与解压:*
# createOutputStream(OutputStream outputStream) 创建了一个CompressionOutputStream
  * 将未压缩的数据写入它
  * 它将压缩过的数据写入底层Stream
# createInputStream(InputStream inputStream) 创建了一个CompressionInputStream
  * 从它读取未压缩过的数据
  * 底层Stream返回的实际上是压缩过的数据
   
*使用CompressionCodecFactory推断CompressionCodec:*
# 一般可以通过文件扩展名推断压缩算法
# CompressionCodecFactory的方法getCodec(Path), 根据扩展名获得对应的Codec
# 属性 *io.compression.codecs:*
  * 定义了一个列表， CompressionCodecFactory 就是从这里面查找对应的Codec
  * 默认列出hadoop支持的所有的codec
  * 可以向里面注册自己自定义的
   
*原生类库Native Library:*
为了性能，推荐使用原生类库进行解压和压缩。

# 并不是所有的压缩方式都有对应的原生类库
# 如果找不到原生类库，就使用java实现 
# 原生类库默认在 `lib/native` 下
# 原生类库位置可以通过java系统的属性指定：java.library.path
# 默认hadoop会自动搜索自己运行所在平台的原生类库
# 搜到自动加载
# 可以通过属性haddop.native.lib禁止使用原生类库，设为false
# 如果使用原生库且做大量压缩与解压， 通过 *CodecPool* 可以重复使用 *Compressor* 实例，提升性能：
  * 就像线程池一样，有借有还
  * 借：CodecPool.getCompressor(CompressionCodec)
  * 还：CodecPool.returnCompressor(Compressor)
    {{{ class="brush:java"
          public static void main(String[] args) throws Exception {
              String codecClassname = args[0];
              Class<?> codecClass = Class.forName(codecClassname);
              Configuration conf = new Configuration();
              CompressionCodec codec = (CompressionCodec)
                  CodecPool.ReflectionUtils.newInstance(codecClass, conf);
              Compressor compressor = null;
              try {
                  compressor = CodecPool.getCompressor(codec);
                  CompressionOutputStream out =
                      codec.createOutputStream(System.out, compressor);
                  IOUtils.copyBytes(System.in, out, 4096, false);
                  out.finish();
              } finally {
                  CodecPool.returnCompressor(compressor);
              }
          }
      }}}
      
===压缩与输入切片Compression and Input Splitting ===
当被压缩的数据要被MapReduce使用时，该压缩算法是否支持切片将变的十分重要.
* 如果不支持切片，单个数据块就无法作为数据切片供 MapReduce使用
* 但可以将整个文件的所有数据块作为单个Map任务的输入，会失去MapReduce的优势

*使用哪种压缩格式？ *
* 与具体的应用有关, 要考虑文件的大小，格式，处理所用的工具
* 下面有一些方案，效率有高到低：
  # 使用Sequence File或Avro datafile. 两者都支持压缩与切片，选择一个快速算法：LZO, LZ4, Snappy
  # 使用一种支持切片的压缩格式，比如bzip2,或者LZO(可以通过建立切分点索引以支持切片）
  # application将文件分成一个个的chunk，每个chunk单独压缩存储， 每个chunk压缩后的大小应该近似于HDFS的block
  # 不压缩直接存储
* 不应该大文件使用不支持切分的压缩格式，MapReduce将失去优势

===在MapReduce中使用压缩Using Compression in MapReduce===
*输入:*
# 一个压缩的文件作为MapReduce的输入，会自动进行解压
# 使用文件扩展名来确定所使用的Codec

*输出:*
# 可以确定是否对MapReduce的输出进入压缩。
# 通过属性 `mapred.output.compress` 来指定， true or false
# 通过属性 `mapred.output.compression.codec` 来指定所使用的Codec
# 如果不通过属性指定，可以通过FileOutputFormat的静态方法来指定
{{{ class="brush:java"
    FileOutputFormat.setCompressOutput(job, true);
    FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);
  }}}


*MapReduce compression properties:*
| Property name                   | Type        | Default  value                             | Description                                                                     |   |
|---------------------------------|-------------|--------------------------------------------|---------------------------------------------------------------------------------|---|
| mapred.output.compress          | boolean     | false                                      | Compress outputs                                                                |   |
| mapred.output.compression.codec | Class  name | org.apache.hadoop.io.compress.DefaultCodec | The compression codec to use for outputs                                        |   |
| mapred.output.compression.type  | String      | RECORD                                     | The type of compression to use for SequenceFile outputs: NONE, RECORD, or BLOCK |   |

*压缩Map输出Compressing Map Output:*
# 对Map task产生的中间结果进行压缩也是可以获得好处的
# Map的输出会存储到磁盘上，并通过网络传输到Reduce结点
# 如果使用快速压缩算法LZO LZ4 或Snappy,就可以获得性能提升
# 相关属性：
| Property name                       | Type    | Default value                              | Description                                 |
|-------------------------------------|---------|--------------------------------------------|---------------------------------------------|
| mapred.compress.map.output          | boolean | false                                      | Compress map outputs                        |
| mapred.map.output.compression.codec | Class   | org.apache.hadoop.io.compress.DefaultCodec | The compression codec to use for map output |

* 代码示例：
{{{ class="brush:java"
    Configuration conf = new Configuration();
    conf.setBoolean("mapred.compress.map.output", true);
    conf.setClass("mapred.map.output.compression.codec", GzipCodec.class,
            CompressionCodec.class);
    Job job = new Job(conf);
  }}}

==序列化Serialization==
*序列化Serialization:* 将结构化数据转化为字节流

*反序列化Deserailization:* 将字节流转化回结构化数据

序列化经常用于两个方面 *进程间通信Interproess communication* 和 *永久存储Persistent storage*

Hadoop使用RPC进行不同结点间的定义，RPC协议对消息进行Serialization和Deserialization

不管通信还是永久存储对序列化格式都有如下要求（但解释的侧重点不同）：
* Compact 紧凑
* Fast 快速
* Extensible 可扩展
* Interoprable 互操作性

Hadoop使用自己的序列化格式Writable, 紧凑，快速，但很难被其他的语言所扩展

===Writable接口 ===
*两个方法:*
{{{ class="brush:java"
    public interface Writable {
        void write(DataOutput out) throws IOException;
        void readFields(DataInput in) throws IOException;
    }
  }}}
  
*WritableComparable and comparators:*
* 在MapReduce的排序阶段，类型比较是十分重要的
* WritableComparable
  {{{ class="brush:java"
        package org.apache.hadoop.io;
        public interface WritableComparable<T> extends Writable, Comparable<T> {
        }
    }}}
* RawComparator
  # 对Comparator的一个扩展
  # 允许实现不用反序列化，基于就可以对记录进行比较
  # 定义
  {{{ class="brush:java"
        package org.apache.hadoop.io;
        import java.util.Comparator;
        public interface RawComparator<T> extends Comparator<T> {
            public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2);
        }
    }}}
* WritableComparator
  # RawComparator的一个通用实现
  # 功能一：实现compare()方法
    * 从流中将对象返序列化
    * 调用对象的compareTo()方法进行比较
  # 功能二：作为RawComparator的实例工厂
    * 用于获取指定Writable的Comparator
      {{{ class="brush:java"
          //获取用于IntWritable的Comparator
          RawComparator<IntWritable> comparator = WritableComparator.get(IntWritable.class);
        }}}
    * 获取Comparator实例后，也可以不反序列化，直接基于流比较
      {{{ class="brush:java"
            IntWritable w1 = new IntWritable(163);
            IntWritable w2 = new IntWritable(67);
            assertThat(comparator.compare(w1, w2), greaterThan(0));
            
            //OR:
            
            byte[] b1 = serialize(w1);
            byte[] b2 = serialize(w2);
            assertThat(comparator.compare(b1, 0, b1.length, b2, 0, b2.length),
                    greaterThan(0));
        }}}

===Writable类 ===
在 `org.apache.hadoop.io` 里有丰富的Writable类。
{{hadoop-writables.png}}

*整数编码:*
# 定长格式: `IntWritable`  and `LongWritable`
# 变长格式: `VIntWritable` and `VLongWritable`
# 变长格式会做优化，比如数很小时(-127 ~ 127), 就会只有一个字节保存 

*Text:*
# 是针对UTF-8字符序列的Writable类
# 索引
# 迭代
# 易变，可重用
 
*BytesWritable:*
# 对一个二进制字节数组的包装
# 有两部分组成
  * 一个4字节的整数，指明数组的长度
  * 数组中的字节
   
*NullWritable:*
# 特殊类型
# 长度为0
# 不从流中读数据，也不写数据
# 充当点位符
# 单例
# 通过 `NullWritable.get()a` 获得
 
*ObjectWritable:*
# 对java基本类型或基本类型数组的一个通用封装(primitives,String, enum, Writable, null)
# Hadoop RPC中对参数或返回值进行封装和解封
# 当一个field包含多个类型时非常有用
  * 把这个段的值映射为一个ObjectWritable
  * 这会浪费一些空间，序列化时需要指定值的类型，也就是类型的Classname
# 使用 *GenericWritable* 进行优化
  * 如果这个field的可能的类型是一个已知的类型数组
  * 可以声明这样一个数组
  * 序列化field值时，通过索引指定值的类型，而不是classname, 可以提高性能
  * 需要创建一个子类，声明支持的类型

*Writable集合类:*
# 有六个: `ArrayWritable, ArrayPrimitiveWritable, TwoDArrayWritable, MapWritable, SortedMapWritable, EnumSetWritable`.
# ArrayWritable和TowDArrayWritable
  * 分别用于Writable的数组和二维数组
  * 里面的 element必须是同类型的Writable
  * 用于SequenceFile的键或值，或MapReduce的输入
  * 创建它位的子类，以指定类型
    {{{ class="brush:java"
          public class TextArrayWritable extends ArrayWritable {
              public TextArrayWritable() {
                  super(Text.class);
              }
          }
      }}}
# ArrayPrimitiveWritable
  * 用于java的primitive数组
  * 类型通过set()时自检测
# MapWritable和SortedMapWritable
  * 分别是java.util.Map<Writable,Writable>和java.util.SortedMap<WritableComparable,Writable>的实现
  * key和value的类型
    * 作为这个Field序列化一部分存在的
    * 一个字节
    * 指定一个类型数组的索引
    * 这个数组由org.apache.hadoop.io中标准类型填充
    * 可以扩展自定义Writable类型，通过写一个header实现，里面封装了非标准Writable类型的数组
    
*自定义Writable类型:*
# 标准的Writable的类型已经可以满足大部分需求
# 仍然可以自定义Writable, 对二进制表示和排序实现全控制
# Writable是MapReduce的核心
# 调整Writable的二进制表示对性能有很大的影响
# 标准的Writable已经有了很好的性能
# 有时对于复杂的结构创建一个自定义的Writable会有更好的性能

===序列化框架 ===
# MapReduce并不强迫使用Writable进行序列化
# 反而提供了一种 *可替换序列化框架的机制 * 
  * 接口 `org.apache.hadoop.io.serializer.Serialization`
  * 接口的每个实现就是一个 序列化框架
  * 定义了由Type到Serializer和Deserailizer的映射
  * 比如WritableSerialization和JavaSerialization(java默认机制）
# 通过属性 `io.serializations` 指定支持的 序列化框架
  * 值是一个 全类名列表
  * 默认值：org.apache.hadoop.io.serialization.WritableSerialization
  * 所以默认由Writable进行序列化
# 为什么不用Java Serialization
  * 精确控制对象的读和写，就hadoop的核心
  * 需要精简  快速  可扩展 可互操作
  * java serialization复杂
  * 它基于引用，需要存储类名和状态
# 为什么不用RMI
  * 进程间通信也是hadoop的核心
  * 需要精确控制连接，延迟和缓冲的处理方式，RMI无法满足
 
*序列第IDL Interface Description Language*
# 有些框架不依赖语言来解决问题，而是通过IDL来定义相关信息
# Thrift 和 Protocaol Buffers两个比较流行的序列化框架

==AVRO==
# 独立于编程语言的数据序列化系统
# Avro数据用语言无关的Schema定义
# Avro代码生成是可选的
# 可以根据一个Schema进行读写数据，哪怕第一次见到这个Schema
# Avro假定Schema在读写时总是存在的
# 这会生成非常紧凑的编码数据，因为在编码的数据中不需要指定字段标识符
# Avro通常使用json编写schema, 使用二进制对数据进行编码
# 还有其他可选项
# Avro有很强的schema解析能力，读写时的schema不必完全一致，它们会分别忽略掉自己不关心的内容
# 支持压缩可切分
 
===核心类 ===
# Schema
# Schema.Parser 解析一个文件，用于生成一个Schema实例
# GenericRecord 代表一个要持久化的对象, 由Schema而来
# DatumWriter<T>  输出序列化
# Encoder
# DatumReader<T>  反序列化
# Encoder
{{{ class="brush:java"
    public static void main(String[] args) throws IOException {
        Schema.Parser parser = new Schema.Parser();
        Schema schema = parser.parse(Demo.class.getResourceAsStream("/StringPair.avsc"));


        GenericRecord datum = new GenericData.Record(schema);
        datum.put("left", "L");
        datum.put("right", "R");

        ByteArrayOutputStream out = new ByteArrayOutputStream();
        DatumWriter<GenericRecord> writer = new GenericDatumWriter<GenericRecord>(schema);
        Encoder encoder = EncoderFactory.get().binaryEncoder(out, null);
        writer.write(datum, encoder);
        encoder.flush();
        out.close();

        DatumReader<GenericRecord> reader = new GenericDatumReader<GenericRecord>(schema);
        Decoder decoder = DecoderFactory.get().binaryDecoder(out.toByteArray(), null);
        GenericRecord result = reader.read(null, decoder);
        assertThat(result.get("left").toString(), is("L"));
        assertThat(result.get("right").toString(), is("R"));
        System.out.println(result.get("left").toString());
    }
  }}}
  
===Avro Data Files ===
# 用于存储Avro 对象序列
# 文件头会存储schema的meta data
# 跨语言
# 可切分，适合MapReduce快速处理
# 和写入流类似
# 使用DataFileWriter和DataFileReader
# append的对象必须符合schema，不然会报错
{{{ class="brush:java"

    File file = new File("data.avro");
    DatumWriter<GenericRecord> writer = new GenericDatumWriter<GenericRecord>(schema);
    DataFileWriter<GenericRecord> dataFileWriter = new DataFileWriter<GenericRecord>(writer);
    dataFileWriter.create(schema, file);
    dataFileWriter.append(datum);
    dataFileWriter.close();

    DatumReader<GenericRecord> reader = new GenericDatumReader<GenericRecord>();
    DataFileReader<GenericRecord> dataFileReader =
    new DataFileReader<GenericRecord>(file, reader);
    assertThat("Schema is the same", schema, is(dataFileReader.getSchema()));
    

    GenericRecord record = null;
    while (dataFileReader.hasNext()) {
        record = dataFileReader.next(record);
        // process record
    }

  }}}

===Schemo Resolution ===
# 支持写入的Schema和读入的Schema不同
# 在Schema中增加的字段必须增加默认值
# 在Schema中可以使用Alias,可以使读时的字段名和写入时的字段名不一致
# Rules of the resolution:

| New schema    | Writer | Reader | Action                                                                                                                                                                                                                                                                 |
|---------------|--------|--------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Added field   | Old    | New    | The reader uses the default value of the new field, since it is not written by the writer.                                                                                                                                                                             |
| \/            | New    | Old    | The reader does not know about the new field written by the writer, so it is ignored(projection).                                                                                                                                                                      |
| Removed field | Old    | New    | The reader ignores the removed field (projection).                                                                                                                                                                                                                     |
| \/            | New    | Old    | The removed field is not written by the writer. If the old schema had a default defined for the field, the reader uses this; otherwise, it gets an error. In this case, it is best to update the reader’s schema, either at the same time as or before the writer’s. |

===Sort Order ===
# 除Record之外的Type具有预定义的规则进行排序，大多是自然顺序排序，且用户不能修改
# 对于Record用户可以指定 field进行排序 , 默认是按fied进行定义的顺序依次进行比较
# 实现了高效的二进制比较，即不必反序列化就可以进行比较
 
==基于文件的数据结构 File-Based Data Structure==

===SequenceFile===
* 提供一种持久化存储 *二进制键值对* 的数据结构
* 用其包装小文件，可对提高对小文件的处理效率，因为hadoop和mapreduce对处理大文件进行了优化

*Writing a SequenceFile:*
# 关键：FileSystem, Path, Configuration, SequenceFile.Writer, SequenceFile.createWriter(...)
# writer.getLength()返回文件的当前位置 
{{{ class="brush:java"
    public class SequenceFileWriteDemo {
        private static final String[] DATA = {
            "One, two, buckle my shoe",
            "Three, four, shut the door",
            "Five, six, pick up sticks",
            "Seven, eight, lay them straight",
            "Nine, ten, a big fat hen"
        };
        public static void main(String[] args) throws IOException {
            String uri = args[0];
            Configuration conf = new Configuration();
            FileSystem fs = FileSystem.get(URI.create(uri), conf);
            Path path = new Path(uri);
            IntWritable key = new IntWritable();
            Text value = new Text();
            SequenceFile.Writer writer = null;
            try {
                writer = SequenceFile.createWriter(fs, conf, path,
                        key.getClass(), value.getClass());
                for (int i = 0; i < 100; i++) {
                    key.set(100 - i);
                    value.set(DATA[i % DATA.length]);
                    System.out.printf("[%s]\t%s\t%s\n", writer.getLength(), key, value);
                    writer.append(key, value);
                }
            } finally {
                IOUtils.closeStream(writer);
            }
        }
    }
  }}}

*Reading a SequenceFlie:*
# 非Writable序列化的使用下列两个方法：
  * public Object next(Object key) throws IOException
  * public Object getCurrentValue(Object val) throws IOException
# 如果使用了压缩，记得正确设置属性io.serializations
# *sync point:*
  # sync point 就是在流中一个可以重新同步的位置，与记录的边界的对齐
  # SequenceFile.sync()每隔一段时间会插入一个特殊项sync point
  # 有两种方法可以到达sequencefile的指定文置：
  # seek(position):到达文件指定的位置，但这个位置不是记录边界，读数据时会异常
  # sync(position):到达position后的第一个sync点，如果没有到达末尾
{{{ class="brush:java"
    public class SequenceFileReadDemo {
        public static void main(String[] args) throws IOException {
            String uri = args[0];
            Configuration conf = new Configuration();
            FileSystem fs = FileSystem.get(URI.create(uri), conf);
            Path path = new Path(uri);
            SequenceFile.Reader reader = null;
            try {
                reader = new SequenceFile.Reader(fs, path, conf);
                Writable key = (Writable)
                    ReflectionUtils.newInstance(reader.getKeyClass(), conf);
                Writable value = (Writable)
                    ReflectionUtils.newInstance(reader.getValueClass(), conf);
                long position = reader.getPosition();
                while (reader.next(key, value)) {
                    String syncSeen = reader.syncSeen() ? "*" : ";
                    System.out.printf("[%s%s]\t%s\t%s\n", position, syncSeen, key, value);
                    position = reader.getPosition(); // beginning of next record
                }
            } finally {
                IOUtils.closeStream(reader);
            }
        }
    }
  }}}
  
*Displaying a SequenceFlie with a command-line interface:*
# `hadoop fs -text`
# 可以识别gzipped files和sequence files, 其它的会假定为plain text

*Sorting and Merging SequenceFiles:*
# 使用MapReduce
# 使用比MapReduce更底层的：SequenceFile.Sorter

*The SequenceFile format:*
# 文件由一个Header后跟一个或多个Record组成
# Header:
  * 前三个字节是SEQ，作为magic number
  * 第四个字节是version number
  * 其他字段：the names of the key and value classes, compression details, user-defined metadata, sync marker.
# 后面Record的存储与是否启用压缩相关，启用压缩又与是record压缩还是block压缩相关
# 无压缩
  * record由四部分组成：record length, key length, key, value
  * length由四字节组成
# Record压缩：
  * 和无压缩的结构一样
  * 只是value通过header中指定的压缩方式压缩过了
  * key是无压缩的
# Block压缩：
  * 一次压缩多条record,放入一个block中，压缩效率高，更紧凑
  * 可以向一个block中不断压入record，直到它的大小达到一个最小值，通过`io.seqfile.compress.blocksize`指定
  * 每一个block后会插入一个sync marker
  * 一个block的内部结构由五部分组成(后四个为压缩数据）：nubmer of records, the key lengths, the keys, the value lengths, the values. 
{{./sequence-file-format.png}}

*Block Compression:*

{{./sequence-file-block-compression.png}}


===MappedFile===
# MappedFile就是排好序的SequenceFile
# 有两个文件组成：一个是数据文件，一个索引文件index file
# index file中存放key和key在数据文件中的postion的键值对，index file中key是排好序的
# 并不是每一个key都存在index　file中，因为index file会整个加载进内存。
# 通过设置一个interval属性，来决定每隔几个key，存一个key到index文件中。
# 当根据key从文件中获取值时，先从index file中获取最接近key的位置，从这个位置开始查找
