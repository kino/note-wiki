<!DOCTYPE html>
<html>
<head>

<title>Hadoop I/O</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">

<script type="text/javascript" src="../../js/jquery-1.6.4.min.js"></script>

<link rel="Stylesheet" type="text/css" href="../../js/sh/styles/shCore.css">
<link rel="Stylesheet" type="text/css" href="../../js/sh/styles/shThemeRDark.css">
<script type="text/javascript" src="../../js/sh/scripts/shCore.js"></script>
<script type="text/javascript" src="../../js/sh/scripts/shAutoloader.js"></script>

<script type="text/javascript" src="../../js/main.js"></script>
<script type="text/javascript" src="../../js/ASCIIMathML.js"></script>

<link rel="Stylesheet" type="text/css" href="../../style.css">
<link rel="Stylesheet" type="text/css" href="../../css/main.css">

</head>
<body>
	<div class="hidden">
		<input id="root_path" type="text" value="../../">
	</div>
	<div id="body-wrapper">
		<div id="container">
			<div id="top">
				<div id="page-title">
					<a href="../../index.html">烂笔头</a>
				</div>
				<ul id="top-nav">
				</ul>
			</div>
			<div id="middle">
				
<div class="toc">
<ul>
<li><a href="#toc_1">Hadoop I/O</a>
<ul>
<li><a href="#toc_1.1">数据完整性 Integrity</a>
<ul>
<li><a href="#toc_1.1.1">HDFS中的数据完整性 </a>
<li><a href="#toc_1.1.2">LocalFileSystem类 </a>
<li><a href="#toc_1.1.3">ChecksumFileSystem类 </a>
</ul>
<li><a href="#toc_1.2">文件压缩File Compression</a>
<ul>
<li><a href="#toc_1.2.1">Codecs</a>
<li><a href="#toc_1.2.2">压缩与输入切片Compression and Input Splitting</a>
<li><a href="#toc_1.2.3">在MapReduce中使用压缩Using Compression in MapReduce</a>
</ul>
<li><a href="#toc_1.3">序列化Serialization</a>
<ul>
<li><a href="#toc_1.3.1">Writable接口</a>
<li><a href="#toc_1.3.2">Writable类 </a>
<li><a href="#toc_1.3.3">序列化框架 </a>
</ul>
<li><a href="#toc_1.4">AVRO</a>
<ul>
<li><a href="#toc_1.4.1">核心类 </a>
<li><a href="#toc_1.4.2">Avro Data Files</a>
<li><a href="#toc_1.4.3">Schemo Resolution</a>
<li><a href="#toc_1.4.4">Sort Order</a>
</ul>
<li><a href="#toc_1.5">基于文件的数据结构 File-Based Data Structure</a>
<ul>
<li><a href="#toc_1.5.1">SequenceFile</a>
</ul>
</ul>
</ul>
</div>
<h1 id="toc_1">Hadoop I/O</h1>

<h2 id="toc_1.1">数据完整性 Integrity</h2>
<p>
为了保证数据不丢失和损坏，在写入或读出数据时进行校验
</p>

<h3 id="toc_1.1.1">HDFS中的数据完整性 </h3>
<p>
<strong>校验和checksum:</strong>
</p>
<ul>
<li>
HDFS的写入数据时计算较验和，在读取时验证校验和

<li>
HDFS对每io.bytes.per.checksum个（default 512)字节计算checksum

<li>
使用CRC-32, checksum为4bytes

<li>
默认存储checksum的额外开销低于1%

</ul>
 
<p>
<strong>写入时:</strong>
</p>
<ol>
<li>
Datanode在存储它接收到的data和它们的checksum前，进行验证

<li>
这些数据来自client或其他datanode

<li>
当client向pipeline写入数据时:

<ul>
<li>
只有最后一个datanode负责验证checksum

<li>
验证出错时，client会收到一个ChecksumException(IOExcpetion的一个子类)

<li>
由应用程序来处理这个异常，比如重试

</ul>
</ol>
   
<p>
<strong>读取时:</strong>
</p>
<ol>
<li>
client读取数据也要进行checksum的验证

<li>
重新计算checksum并和datanode上存储的checksum进行比较

<li>
datanode还存储了一个checksum的验证日志，这存储最后一次进行checksum验证的时间

<li>
client验证checksum成功后，就会更新这个日志

<li>
保留这份统计信息对检测损坏磁盘很有价值

</ol>

<p>
<strong>datanode对checksum进行定期验证:</strong>
</p>
<ol>
<li>
datanode使用 <strong>DataBlockScanner</strong>

<li>
对存储在其上的所有的Block进行定期验证

<li>
可以有效防止由于bit rot造成的物理磁盘损坏

</ol>

<p>
<strong>修复损坏的block:</strong>
</p>
<ol>
<li>
可以通过复制一个正常的block产生一个新的正常的block复本

<li>
当client检测到一个checksum  error时

<li>
在抛出异常前，client会向namenode报告出现error的block和其所在datanode

<li>
namenode并不会指挥client去修复这个error

<li>
namenode会异步的计划在另外的datanode重建一个这个block的复本，使其复本数量达到设置的水平

</ol>

<p>
<strong>关闭checksum验证:</strong>
</p>
<ul>
<li>
使用open()读取一个文件前调用FileSystem的setVerifyChecksum(false)方法

<li>
在shell中使用-get和-copyToLocal时添加-ignoreCrc选项

<ul>
<li>
可以将损坏的文件拷到本地进行检查，很有用的

</ul>
</ul>

<h3 id="toc_1.1.2">LocalFileSystem类 </h3>

<p>
Hadoop中的LocalFileSystem类负责执行客户端的文件验证
</p>
<ol>
<li>
当创建一个 <code>filename</code> 的文件时

<li>
客户端会在同一目录下同时创建一个隐藏文件 <code>.filename.crc</code>, 这种行为是透明的

<ul>
<li>
这个文件包含了对于文件 <code>filename</code> 每一个chunk的checksum

<li>
chunk的大小同样由io.bytes.per.checksum指定

<li>
这个文件还同时存储了这次验证所使用的chunk大小(作为元数据），即当前io.bytes.per.checksum的大小

<li>
因此以后即使修改了io.bytes.per.checksum的配置，也不影响恢复文件

</ul>
<li>
客户端在读取文件时，会验证Checksums, 如果发生error, LocalFileSystem会抛出一个ChecksumException

</ol>
 
<p>
<strong>禁止验证</strong> 
</p>

<p>
可以禁止验证，特别是当底层系统已经支持的情况下，这时可以 <strong>使用RawLocalFileSystem代替LocalFilesystem</strong>
</p>
<ul>
<li>
在应用全局内生效：将属性fs.file.impl设置为org.apache.hadoop.fs.RawLocalFileSysem, 它实现了对文件URI的重新映射

<li>
某次读取使用，可以生成一个RawLocalFileSystem实例
<pre  class="brush:java">
      Configuration conf = ...
      FileSystem fs = new RawLocalFileSystem();
      fs.initialize(null, conf);
</pre>

</ul>
  
<h3 id="toc_1.1.3">ChecksumFileSystem类 </h3>

<p>
LocalFileSystem使用ChecksumFileSystem去做这它的工作（晕死，有完没完)
</p>

<p>
ChecksumFileSystem很容易给其他的文件系统添加文件验证的功能，它其实是FileSystem类的一个包装类Wrapper
</p>
<pre  class="brush:java">
    FileSystem rawFs = ...
    FileSystem checksummedFs = new ChecksumFileSystem(rawFs);
</pre>
  
<ol>
<li>
rawFs是具体的底层文件系统

<li>
ChecksumFileSystem的getRawFileSystem()可以获取它

<li>
ChecksumFileSysem的getChecksumFile()可以获得任一个文件校验文件checksumfile的路径

<li>
发生错误时，会调用ChecksumFileSysem的reportCheckFailure(), 默认什么也不做

<ul>
<li>
LocalFileSystem对这个方法的实现 ：

<li>
将出错文件 和它的校验文件移到同一设备的一个side directory , 名叫bad_files

<li>
管理员应该定期检查这个文件夹

</ul>
</ol>
  
<h2 id="toc_1.2">文件压缩File Compression</h2>
<p>
<strong>文件压缩的两大好处:</strong>
</p>
<ul>
<li>
减少存储空间

<li>
加快传输

</ul>
 
<p>
<strong>常用压缩格式:</strong>
<table>
<tr>
<th>
Compression format
</th>
<th>
Tool
</th>
<th>
Algorithm
</th>
<th>
Filename extension
</th>
<th>
Splittable?
</th>
</tr>
<tr>
<td>
DEFLATE
</td>
<td>
N/A
</td>
<td>
DEFLATE
</td>
<td>
.deflate
</td>
<td>
No
</td>
</tr>
<tr>
<td>
gzip
</td>
<td>
gzip
</td>
<td>
DEFLATE
</td>
<td>
.gz
</td>
<td>
No
</td>
</tr>
<tr>
<td>
bzip2
</td>
<td>
bzip2
</td>
<td>
bzip2
</td>
<td>
.bz2
</td>
<td>
Yes
</td>
</tr>
<tr>
<td>
LZO
</td>
<td>
lzop
</td>
<td>
LZO
</td>
<td>
.lzo
</td>
<td>
No(可以预建切分索引，以支持切分） 
</td>
</tr>
<tr>
<td>
LZ4
</td>
<td>
N/A
</td>
<td>
LZ4
</td>
<td>
.lz4
</td>
<td>
No
</td>
</tr>
<tr>
<td>
Snappy
</td>
<td>
N/A
</td>
<td>
Snappy
</td>
<td>
.snappy
</td>
<td>
No
</td>
</tr>
</table>
</p>

<h3 id="toc_1.2.1">Codecs</h3>
<ul>
<li>
一个codec代表一种压缩解压算法 

<li>
一个codec是接口CompressionCodec的一个实现

</ul>
<table>
<tr>
<th>
Compression format
</th>
<th>
Hadoop CompressionCodec
</th>
</tr>
<tr>
<td>
DEFLATE
</td>
<td>
org.apache.hadoop.io.compress.DefaultCodec
</td>
</tr>
<tr>
<td>
gzip
</td>
<td>
org.apache.hadoop.io.compress.GzipCodec
</td>
</tr>
<tr>
<td>
bzip2
</td>
<td>
org.apache.hadoop.io.compress.BZip2Codec
</td>
</tr>
<tr>
<td>
LZO
</td>
<td>
com.hadoop.compression.lzo.LzopCodec
</td>
</tr>
<tr>
<td>
LZ4
</td>
<td>
org.apache.hadoop.io.compress.Lz4Codec
</td>
</tr>
<tr>
<td>
Snappy
</td>
<td>
org.apache.hadoop.io.compress.SnappyCodec
</td>
</tr>
</table>

<p>
<strong>使用CompressionCodec对流进行压缩与解压:</strong>
</p>
<ol>
<li>
createOutputStream(OutputStream outputStream) 创建了一个CompressionOutputStream

<ul>
<li>
将未压缩的数据写入它

<li>
它将压缩过的数据写入底层Stream

</ul>
<li>
createInputStream(InputStream inputStream) 创建了一个CompressionInputStream

<ul>
<li>
从它读取未压缩过的数据

<li>
底层Stream返回的实际上是压缩过的数据

</ul>
</ol>
   
<p>
<strong>使用CompressionCodecFactory推断CompressionCodec:</strong>
</p>
<ol>
<li>
一般可以通过文件扩展名推断压缩算法

<li>
CompressionCodecFactory的方法getCodec(Path), 根据扩展名获得对应的Codec

<li>
属性 <strong>io.compression.codecs:</strong>

<ul>
<li>
定义了一个列表， CompressionCodecFactory 就是从这里面查找对应的Codec

<li>
默认列出hadoop支持的所有的codec

<li>
可以向里面注册自己自定义的

</ul>
</ol>
   
<p>
<strong>原生类库Native Library:</strong>
为了性能，推荐使用原生类库进行解压和压缩。
</p>

<ol>
<li>
并不是所有的压缩方式都有对应的原生类库

<li>
如果找不到原生类库，就使用java实现 

<li>
原生类库默认在 <code>lib/native</code> 下

<li>
原生类库位置可以通过java系统的属性指定：java.library.path

<li>
默认hadoop会自动搜索自己运行所在平台的原生类库

<li>
搜到自动加载

<li>
可以通过属性haddop.native.lib禁止使用原生类库，设为false

<li>
如果使用原生库且做大量压缩与解压， 通过 <strong>CodecPool</strong> 可以重复使用 <strong>Compressor</strong> 实例，提升性能：

<ul>
<li>
就像线程池一样，有借有还

<li>
借：CodecPool.getCompressor(CompressionCodec)

<li>
还：CodecPool.returnCompressor(Compressor)
<pre  class="brush:java">
          public static void main(String[] args) throws Exception {
              String codecClassname = args[0];
              Class&lt;?&gt; codecClass = Class.forName(codecClassname);
              Configuration conf = new Configuration();
              CompressionCodec codec = (CompressionCodec)
                  CodecPool.ReflectionUtils.newInstance(codecClass, conf);
              Compressor compressor = null;
              try {
                  compressor = CodecPool.getCompressor(codec);
                  CompressionOutputStream out =
                      codec.createOutputStream(System.out, compressor);
                  IOUtils.copyBytes(System.in, out, 4096, false);
                  out.finish();
              } finally {
                  CodecPool.returnCompressor(compressor);
              }
          }
</pre>

</ul>
</ol>
      
<h3 id="toc_1.2.2">压缩与输入切片Compression and Input Splitting</h3>
<p>
当被压缩的数据要被MapReduce使用时，该压缩算法是否支持切片将变的十分重要.
</p>
<ul>
<li>
如果不支持切片，单个数据块就无法作为数据切片供 MapReduce使用

<li>
但可以将整个文件的所有数据块作为单个Map任务的输入，会失去MapReduce的优势

</ul>

<p>
<strong>使用哪种压缩格式？ </strong>
</p>
<ul>
<li>
与具体的应用有关, 要考虑文件的大小，格式，处理所用的工具

<li>
下面有一些方案，效率有高到低：

<ol>
<li>
使用Sequence File或Avro datafile. 两者都支持压缩与切片，选择一个快速算法：LZO, LZ4, Snappy

<li>
使用一种支持切片的压缩格式，比如bzip2,或者LZO(可以通过建立切分点索引以支持切片）

<li>
application将文件分成一个个的chunk，每个chunk单独压缩存储， 每个chunk压缩后的大小应该近似于HDFS的block

<li>
不压缩直接存储

</ol>
<li>
不应该大文件使用不支持切分的压缩格式，MapReduce将失去优势

</ul>

<h3 id="toc_1.2.3">在MapReduce中使用压缩Using Compression in MapReduce</h3>
<p>
<strong>输入:</strong>
</p>
<ol>
<li>
一个压缩的文件作为MapReduce的输入，会自动进行解压

<li>
使用文件扩展名来确定所使用的Codec

</ol>

<p>
<strong>输出:</strong>
</p>
<ol>
<li>
可以确定是否对MapReduce的输出进入压缩。

<li>
通过属性 <code>mapred.output.compress</code> 来指定， true or false

<li>
通过属性 <code>mapred.output.compression.codec</code> 来指定所使用的Codec

<li>
如果不通过属性指定，可以通过FileOutputFormat的静态方法来指定
<pre  class="brush:java">
    FileOutputFormat.setCompressOutput(job, true);
    FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);
</pre>

</ol>


<p>
<strong>MapReduce compression properties:</strong>
<table>
<tr>
<th>
Property name
</th>
<th>
Type
</th>
<th>
Default  value
</th>
<th>
Description
</th>
<th>
&nbsp;
</th>
</tr>
<tr>
<td>
mapred.output.compress
</td>
<td>
boolean
</td>
<td>
false
</td>
<td>
Compress outputs
</td>
<td>
&nbsp;
</td>
</tr>
<tr>
<td>
mapred.output.compression.codec
</td>
<td>
Class  name
</td>
<td>
org.apache.hadoop.io.compress.DefaultCodec
</td>
<td>
The compression codec to use for outputs
</td>
<td>
&nbsp;
</td>
</tr>
<tr>
<td>
mapred.output.compression.type
</td>
<td>
String
</td>
<td>
RECORD
</td>
<td>
The type of compression to use for SequenceFile outputs: NONE, RECORD, or BLOCK
</td>
<td>
&nbsp;
</td>
</tr>
</table>
</p>

<p>
<strong>压缩Map输出Compressing Map Output:</strong>
</p>
<ol>
<li>
对Map task产生的中间结果进行压缩也是可以获得好处的

<li>
Map的输出会存储到磁盘上，并通过网络传输到Reduce结点

<li>
如果使用快速压缩算法LZO LZ4 或Snappy,就可以获得性能提升

<li>
相关属性：

</ol>
<table>
<tr>
<th>
Property name
</th>
<th>
Type
</th>
<th>
Default value
</th>
<th>
Description
</th>
</tr>
<tr>
<td>
mapred.compress.map.output
</td>
<td>
boolean
</td>
<td>
false
</td>
<td>
Compress map outputs
</td>
</tr>
<tr>
<td>
mapred.map.output.compression.codec
</td>
<td>
Class
</td>
<td>
org.apache.hadoop.io.compress.DefaultCodec
</td>
<td>
The compression codec to use for map output
</td>
</tr>
</table>

<ul>
<li>
代码示例：
<pre  class="brush:java">
    Configuration conf = new Configuration();
    conf.setBoolean("mapred.compress.map.output", true);
    conf.setClass("mapred.map.output.compression.codec", GzipCodec.class,
            CompressionCodec.class);
    Job job = new Job(conf);
</pre>

</ul>

<h2 id="toc_1.3">序列化Serialization</h2>
<p>
<strong>序列化Serialization:</strong> 将结构化数据转化为字节流
</p>

<p>
<strong>反序列化Deserailization:</strong> 将字节流转化回结构化数据
</p>

<p>
序列化经常用于两个方面 <strong>进程间通信Interproess communication</strong> 和 <strong>永久存储Persistent storage</strong>
</p>

<p>
Hadoop使用RPC进行不同结点间的定义，RPC协议对消息进行Serialization和Deserialization
</p>

<p>
不管通信还是永久存储对序列化格式都有如下要求（但解释的侧重点不同）：
</p>
<ul>
<li>
Compact 紧凑

<li>
Fast 快速

<li>
Extensible 可扩展

<li>
Interoprable 互操作性

</ul>

<p>
Hadoop使用自己的序列化格式Writable, 紧凑，快速，但很难被其他的语言所扩展
</p>

<h3 id="toc_1.3.1">Writable接口</h3>
<p>
<strong>两个方法:</strong>
</p>
<pre  class="brush:java">
    public interface Writable {
        void write(DataOutput out) throws IOException;
        void readFields(DataInput in) throws IOException;
    }
</pre>
  
<p>
<strong>WritableComparable and comparators:</strong>
</p>
<ul>
<li>
在MapReduce的排序阶段，类型比较是十分重要的

<li>
WritableComparable
<pre  class="brush:java">
        package org.apache.hadoop.io;
        public interface WritableComparable&lt;T&gt; extends Writable, Comparable&lt;T&gt; {
        }
</pre>

<li>
RawComparator

<ol>
<li>
对Comparator的一个扩展

<li>
允许实现不用反序列化，基于就可以对记录进行比较

<li>
定义
<pre  class="brush:java">
        package org.apache.hadoop.io;
        import java.util.Comparator;
        public interface RawComparator&lt;T&gt; extends Comparator&lt;T&gt; {
            public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2);
        }
</pre>

</ol>
<li>
WritableComparator

<ol>
<li>
RawComparator的一个通用实现

<li>
功能一：实现compare()方法

<ul>
<li>
从流中将对象返序列化

<li>
调用对象的compareTo()方法进行比较

</ul>
<li>
功能二：作为RawComparator的实例工厂

<ul>
<li>
用于获取指定Writable的Comparator
<pre  class="brush:java">
          //获取用于IntWritable的Comparator
          RawComparator&lt;IntWritable&gt; comparator = WritableComparator.get(IntWritable.class);
</pre>

<li>
获取Comparator实例后，也可以不反序列化，直接基于流比较
<pre  class="brush:java">
            IntWritable w1 = new IntWritable(163);
            IntWritable w2 = new IntWritable(67);
            assertThat(comparator.compare(w1, w2), greaterThan(0));
            
            //OR:
            
            byte[] b1 = serialize(w1);
            byte[] b2 = serialize(w2);
            assertThat(comparator.compare(b1, 0, b1.length, b2, 0, b2.length),
                    greaterThan(0));
</pre>

</ul>
</ol>
</ul>

<h3 id="toc_1.3.2">Writable类 </h3>
<p>
在 <code>org.apache.hadoop.io</code> 里有丰富的Writable类。
<img src="hadoop-writables.png" />
</p>

<p>
<strong>整数编码:</strong>
</p>
<ol>
<li>
定长格式: <code>IntWritable</code>  and <code>LongWritable</code>

<li>
变长格式: <code>VIntWritable</code> and <code>VLongWritable</code>

<li>
变长格式会做优化，比如数很小时(-127 ~ 127), 就会只有一个字节保存 

</ol>

<p>
<strong>Text:</strong>
</p>
<ol>
<li>
是针对UTF-8字符序列的Writable类

<li>
索引

<li>
迭代

<li>
易变，可重用

</ol>
 
<p>
<strong>BytesWritable:</strong>
</p>
<ol>
<li>
对一个二进制字节数组的包装

<li>
有两部分组成

<ul>
<li>
一个4字节的整数，指明数组的长度

<li>
数组中的字节

</ul>
</ol>
   
<p>
<strong>NullWritable:</strong>
</p>
<ol>
<li>
特殊类型

<li>
长度为0

<li>
不从流中读数据，也不写数据

<li>
充当点位符

<li>
单例

<li>
通过 <code>NullWritable.get()a</code> 获得

</ol>
 
<p>
<strong>ObjectWritable:</strong>
</p>
<ol>
<li>
对java基本类型或基本类型数组的一个通用封装(primitives,String, enum, Writable, null)

<li>
Hadoop RPC中对参数或返回值进行封装和解封

<li>
当一个field包含多个类型时非常有用

<ul>
<li>
把这个段的值映射为一个ObjectWritable

<li>
这会浪费一些空间，序列化时需要指定值的类型，也就是类型的Classname

</ul>
<li>
使用 <strong>GenericWritable</strong> 进行优化

<ul>
<li>
如果这个field的可能的类型是一个已知的类型数组

<li>
可以声明这样一个数组

<li>
序列化field值时，通过索引指定值的类型，而不是classname, 可以提高性能

<li>
需要创建一个子类，声明支持的类型

</ul>
</ol>

<p>
<strong>Writable集合类:</strong>
</p>
<ol>
<li>
有六个: <code>ArrayWritable, ArrayPrimitiveWritable, TwoDArrayWritable, MapWritable, SortedMapWritable, EnumSetWritable</code>.

<li>
ArrayWritable和TowDArrayWritable

<ul>
<li>
分别用于Writable的数组和二维数组

<li>
里面的 element必须是同类型的Writable

<li>
用于SequenceFile的键或值，或MapReduce的输入

<li>
创建它位的子类，以指定类型
<pre  class="brush:java">
          public class TextArrayWritable extends ArrayWritable {
              public TextArrayWritable() {
                  super(Text.class);
              }
          }
</pre>

</ul>
<li>
ArrayPrimitiveWritable

<ul>
<li>
用于java的primitive数组

<li>
类型通过set()时自检测

</ul>
<li>
MapWritable和SortedMapWritable

<ul>
<li>
分别是java.util.Map&lt;Writable,Writable&gt;和java.util.SortedMap&lt;WritableComparable,Writable&gt;的实现

<li>
key和value的类型

<ul>
<li>
作为这个Field序列化一部分存在的

<li>
一个字节

<li>
指定一个类型数组的索引

<li>
这个数组由org.apache.hadoop.io中标准类型填充

<li>
可以扩展自定义Writable类型，通过写一个header实现，里面封装了非标准Writable类型的数组

</ul>
</ul>
</ol>
    
<p>
<strong>自定义Writable类型:</strong>
</p>
<ol>
<li>
标准的Writable的类型已经可以满足大部分需求

<li>
仍然可以自定义Writable, 对二进制表示和排序实现全控制

<li>
Writable是MapReduce的核心

<li>
调整Writable的二进制表示对性能有很大的影响

<li>
标准的Writable已经有了很好的性能

<li>
有时对于复杂的结构创建一个自定义的Writable会有更好的性能

</ol>

<h3 id="toc_1.3.3">序列化框架 </h3>
<ol>
<li>
MapReduce并不强迫使用Writable进行序列化

<li>
反而提供了一种 <strong>可替换序列化框架的机制 </strong> 

<ul>
<li>
接口 <code>org.apache.hadoop.io.serializer.Serialization</code>

<li>
接口的每个实现就是一个 序列化框架

<li>
定义了由Type到Serializer和Deserailizer的映射

<li>
比如WritableSerialization和JavaSerialization(java默认机制）

</ul>
<li>
通过属性 <code>io.serializations</code> 指定支持的 序列化框架

<ul>
<li>
值是一个 全类名列表

<li>
默认值：org.apache.hadoop.io.serialization.WritableSerialization

<li>
所以默认由Writable进行序列化

</ul>
<li>
为什么不用Java Serialization

<ul>
<li>
精确控制对象的读和写，就hadoop的核心

<li>
需要精简  快速  可扩展 可互操作

<li>
java serialization复杂

<li>
它基于引用，需要存储类名和状态

</ul>
<li>
为什么不用RMI

<ul>
<li>
进程间通信也是hadoop的核心

<li>
需要精确控制连接，延迟和缓冲的处理方式，RMI无法满足

</ul>
</ol>
 
<p>
<strong>序列第IDL Interface Description Language</strong>
</p>
<ol>
<li>
有些框架不依赖语言来解决问题，而是通过IDL来定义相关信息

<li>
Thrift 和 Protocaol Buffers两个比较流行的序列化框架

</ol>

<h2 id="toc_1.4">AVRO</h2>
<ol>
<li>
独立于编程语言的数据序列化系统

<li>
Avro数据用语言无关的Schema定义

<li>
Avro代码生成是可选的

<li>
可以根据一个Schema进行读写数据，哪怕第一次见到这个Schema

<li>
Avro假定Schema在读写时总是存在的

<li>
这会生成非常紧凑的编码数据，因为在编码的数据中不需要指定字段标识符

<li>
Avro通常使用json编写schema, 使用二进制对数据进行编码

<li>
还有其他可选项

<li>
Avro有很强的schema解析能力，读写时的schema不必完全一致，它们会分别忽略掉自己不关心的内容

<li>
支持压缩可切分

</ol>
 
<h3 id="toc_1.4.1">核心类 </h3>
<ol>
<li>
Schema

<li>
Schema.Parser 解析一个文件，用于生成一个Schema实例

<li>
GenericRecord 代表一个要持久化的对象, 由Schema而来

<li>
DatumWriter&lt;T&gt;  输出序列化

<li>
Encoder

<li>
DatumReader&lt;T&gt;  反序列化

<li>
Encoder
<pre  class="brush:java">
    public static void main(String[] args) throws IOException {
        Schema.Parser parser = new Schema.Parser();
        Schema schema = parser.parse(Demo.class.getResourceAsStream("/StringPair.avsc"));


        GenericRecord datum = new GenericData.Record(schema);
        datum.put("left", "L");
        datum.put("right", "R");

        ByteArrayOutputStream out = new ByteArrayOutputStream();
        DatumWriter&lt;GenericRecord&gt; writer = new GenericDatumWriter&lt;GenericRecord&gt;(schema);
        Encoder encoder = EncoderFactory.get().binaryEncoder(out, null);
        writer.write(datum, encoder);
        encoder.flush();
        out.close();

        DatumReader&lt;GenericRecord&gt; reader = new GenericDatumReader&lt;GenericRecord&gt;(schema);
        Decoder decoder = DecoderFactory.get().binaryDecoder(out.toByteArray(), null);
        GenericRecord result = reader.read(null, decoder);
        assertThat(result.get("left").toString(), is("L"));
        assertThat(result.get("right").toString(), is("R"));
        System.out.println(result.get("left").toString());
    }
</pre>

</ol>
  
<h3 id="toc_1.4.2">Avro Data Files</h3>
<ol>
<li>
用于存储Avro 对象序列

<li>
文件头会存储schema的meta data

<li>
跨语言

<li>
可切分，适合MapReduce快速处理

<li>
和写入流类似

<li>
使用DataFileWriter和DataFileReader

<li>
append的对象必须符合schema，不然会报错
<pre  class="brush:java">

    File file = new File("data.avro");
    DatumWriter&lt;GenericRecord&gt; writer = new GenericDatumWriter&lt;GenericRecord&gt;(schema);
    DataFileWriter&lt;GenericRecord&gt; dataFileWriter = new DataFileWriter&lt;GenericRecord&gt;(writer);
    dataFileWriter.create(schema, file);
    dataFileWriter.append(datum);
    dataFileWriter.close();

    DatumReader&lt;GenericRecord&gt; reader = new GenericDatumReader&lt;GenericRecord&gt;();
    DataFileReader&lt;GenericRecord&gt; dataFileReader =
    new DataFileReader&lt;GenericRecord&gt;(file, reader);
    assertThat("Schema is the same", schema, is(dataFileReader.getSchema()));
    

    GenericRecord record = null;
    while (dataFileReader.hasNext()) {
        record = dataFileReader.next(record);
        // process record
    }

</pre>

</ol>

<h3 id="toc_1.4.3">Schemo Resolution</h3>
<ol>
<li>
支持写入的Schema和读入的Schema不同

<li>
在Schema中增加的字段必须增加默认值

<li>
在Schema中可以使用Alias,可以使读时的字段名和写入时的字段名不一致

<li>
Rules of the resolution:

</ol>

<table>
<tr>
<th>
New schema
</th>
<th>
Writer
</th>
<th>
Reader
</th>
<th>
Action
</th>
</tr>
<tr>
<td rowspan="2">
Added field
</td>
<td>
Old
</td>
<td>
New
</td>
<td>
The reader uses the default value of the new field, since it is not written by the writer.
</td>
</tr>
<tr>
<td>
New
</td>
<td>
Old
</td>
<td>
The reader does not know about the new field written by the writer, so it is ignored(projection).
</td>
</tr>
<tr>
<td rowspan="2">
Removed field
</td>
<td>
Old
</td>
<td>
New
</td>
<td>
The reader ignores the removed field (projection).
</td>
</tr>
<tr>
<td>
New
</td>
<td>
Old
</td>
<td>
The removed field is not written by the writer. If the old schema had a default defined for the field, the reader uses this; otherwise, it gets an error. In this case, it is best to update the reader’s schema, either at the same time as or before the writer’s.
</td>
</tr>
</table>

<h3 id="toc_1.4.4">Sort Order</h3>
<ol>
<li>
除Record之外的Type具有预定义的规则进行排序，大多是自然顺序排序，且用户不能修改

<li>
对于Record用户可以指定 field进行排序 , 默认是按fied进行定义的顺序依次进行比较

<li>
实现了高效的二进制比较，即不必反序列化就可以进行比较

</ol>
 
<h2 id="toc_1.5">基于文件的数据结构 File-Based Data Structure</h2>

<h3 id="toc_1.5.1">SequenceFile</h3>
<ul>
<li>
提供一种持久化存储 <strong>二进制键值对</strong> 的数据结构

<li>
用其包装小文件，可对提高对小文件的处理效率，因为hadoop和mapreduce对处理大文件进行了优化

</ul>

<p>
<strong>Writing a SequenceFile:</strong>
</p>
<ol>
<li>
关键：FileSystem, Path, Configuration, SequenceFile.Writer, SequenceFile.createWriter(...)

<li>
writer.getLength()返回文件的当前位置 
<pre  class="brush:java">
    public class SequenceFileWriteDemo {
        private static final String[] DATA = {
            "One, two, buckle my shoe",
            "Three, four, shut the door",
            "Five, six, pick up sticks",
            "Seven, eight, lay them straight",
            "Nine, ten, a big fat hen"
        };
        public static void main(String[] args) throws IOException {
            String uri = args[0];
            Configuration conf = new Configuration();
            FileSystem fs = FileSystem.get(URI.create(uri), conf);
            Path path = new Path(uri);
            IntWritable key = new IntWritable();
            Text value = new Text();
            SequenceFile.Writer writer = null;
            try {
                writer = SequenceFile.createWriter(fs, conf, path,
                        key.getClass(), value.getClass());
                for (int i = 0; i &lt; 100; i++) {
                    key.set(100 - i);
                    value.set(DATA[i % DATA.length]);
                    System.out.printf("[%s]\t%s\t%s\n", writer.getLength(), key, value);
                    writer.append(key, value);
                }
            } finally {
                IOUtils.closeStream(writer);
            }
        }
    }
</pre>

</ol>

<p>
<strong>Reading a SequenceFlie:</strong>
</p>
<ol>
<li>
非Writable序列化的使用下列两个方法：

<ul>
<li>
public Object next(Object key) throws IOException

<li>
public Object getCurrentValue(Object val) throws IOException

</ul>
<li>
如果使用了压缩，记得正确设置属性io.serializations

<li>
<strong>sync point:</strong>

<ol>
<li>
sync point 就是在流中一个可以重新同步的位置，与记录的边界的对齐

<li>
SequenceFile.sync()每隔一段时间会插入一个特殊项sync point

<li>
有两种方法可以到达sequencefile的指定文置：

<li>
seek(position):到达文件指定的位置，但这个位置不是记录边界，读数据时会异常

<li>
sync(position):到达position后的第一个sync点，如果没有到达末尾
<pre  class="brush:java">
    public class SequenceFileReadDemo {
        public static void main(String[] args) throws IOException {
            String uri = args[0];
            Configuration conf = new Configuration();
            FileSystem fs = FileSystem.get(URI.create(uri), conf);
            Path path = new Path(uri);
            SequenceFile.Reader reader = null;
            try {
                reader = new SequenceFile.Reader(fs, path, conf);
                Writable key = (Writable)
                    ReflectionUtils.newInstance(reader.getKeyClass(), conf);
                Writable value = (Writable)
                    ReflectionUtils.newInstance(reader.getValueClass(), conf);
                long position = reader.getPosition();
                while (reader.next(key, value)) {
                    String syncSeen = reader.syncSeen() ? "*" : ";
                    System.out.printf("[%s%s]\t%s\t%s\n", position, syncSeen, key, value);
                    position = reader.getPosition(); // beginning of next record
                }
            } finally {
                IOUtils.closeStream(reader);
            }
        }
    }
</pre>

</ol>
</ol>
  
<p>
<strong>Displaying a SequenceFlie with a command-line interface:</strong>
</p>
<ol>
<li>
<code>hadoop fs -text</code>

<li>
可以识别gzipped files和sequence files, 其它的会假定为plain text

</ol>

<p>
<strong>Sorting and Merging SequenceFiles:</strong>
</p>
<ol>
<li>
使用MapReduce

<li>
使用比MapReduce更底层的：SequenceFile.Sorter

</ol>

<p>
<strong>The SequenceFile format:</strong>
</p>
<ol>
<li>
文件由一个Header后跟一个或多个Record组成

<li>
Header:

<ul>
<li>
前三个字节是SEQ，作为magic number

<li>
第四个字节是version number

<li>
其他字段：the names of the key and value classes, compression details, user-defined metadata, sync marker.

</ul>
<li>
后面Record的存储与是否启用压缩相关，启用压缩又与是record压缩还是block压缩相关

<li>
无压缩

<ul>
<li>
record由四部分组成：record length, key length, key, value

<li>
length由四字节组成

</ul>
<li>
Record压缩：

<ul>
<li>
和无压缩的结构一样

<li>
只是value通过header中指定的压缩方式压缩过了

<li>
key是无压缩的

</ul>
<li>
Block压缩：

<ul>
<li>
一次压缩多条record,放入一个block中，压缩效率高，更紧凑

<li>
可以向一个block中不断压入record，直到它的大小达到一个最小值，通过<code>io.seqfile.compress.blocksize</code>指定

<li>
每一个block后会插入一个sync marker

<li>
一个block的内部结构由五部分组成(后四个为压缩数据）：nubmer of records, the key lengths, the keys, the value lengths, the values. 

</ul>
</ol>
<p>
<img src="./sequence-file-format.png" />
</p>

<p>
<strong>Block Compression:</strong>
</p>

<p>
<img src="./sequence-file-block-compression.png" />
</p>

			</div>

			<div id="bottom">
				&copy; 2012 王兴朝
			</div>
		</div>
	<div>
</body>
</html>
